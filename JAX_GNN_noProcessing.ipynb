{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import flax.nnx as nnx\n",
    "from flax import struct\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import jraph\n",
    "from itertools import combinations\n",
    "import meshio\n",
    "import numpy as np\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78afd1",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b53ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 50\n",
    "alpha = 1.0\n",
    "gamma = 1.0\n",
    "lambda_ = 1.0\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "batch_size = 1\n",
    "train_split = 0.9\n",
    "CV_split = 0.05\n",
    "test_split = 0.05\n",
    "Learn_Rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd846b",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f457fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992108e",
   "metadata": {},
   "source": [
    "Graph gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_known(boundary_points, points):\n",
    "    is_known = jnp.zeros(points.shape[0]) \n",
    "    is_known = is_known.at[boundary_points].set(1)\n",
    "    return is_known\n",
    "\n",
    "def build_send_receive(cell):\n",
    "    sender_array = []\n",
    "    receiver_array = []\n",
    "    for edge in combinations(cell,2):\n",
    "        sender_array.append(edge[0])\n",
    "        receiver_array.append(edge[1])\n",
    "    return sender_array, receiver_array\n",
    "\n",
    "def build_graphs(senders, receivers, positions, boundary_points, U) -> jraph.GraphsTuple:\n",
    "    is_known = Get_known(boundary_points, positions)\n",
    "    U_applied = jnp.zeros_like(U).at[boundary_points].set(U[boundary_points])\n",
    "        \n",
    "    node_features = jnp.concatenate([positions, U_applied, jnp.expand_dims(is_known, axis=1)], axis=1)\n",
    "    num_nodes = positions.shape[0]\n",
    "\n",
    "    graph = jraph.GraphsTuple(\n",
    "        nodes=node_features,\n",
    "        senders=senders,\n",
    "        receivers=receivers,\n",
    "        edges=None,\n",
    "        globals=None, \n",
    "        n_node=jnp.array([num_nodes]),\n",
    "        n_edge=jnp.array([len(senders)])\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab417de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete.\n",
      "\n",
      "Positions array shape: (1331, 3)\n",
      "Boundary indices array shape: (602,)\n",
      "Senders array shape: (14230,)\n",
      "Receivers array shape: (14230,)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your result file\n",
    "filepath = os.path.join('data', 'vtk', 'u_final.vtu')\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Error: '{filepath}' not found. Please check the file path.\")\n",
    "else:\n",
    "    mesh = meshio.read(filepath)\n",
    "\n",
    "    positions = mesh.points\n",
    "    right_face_indices = np.where(np.isclose(positions[:, 0], 1.0))[0]\n",
    "    element_connectivity = mesh.cells[0].data\n",
    "\n",
    "    unique_edges = set()\n",
    "\n",
    "    for element in element_connectivity:\n",
    "        element_senders, element_receivers = build_send_receive(element)\n",
    "        \n",
    "        for i in range(len(element_senders)):\n",
    "            edge = tuple(sorted((element_senders[i], element_receivers[i])))\n",
    "            unique_edges.add(edge)\n",
    "\n",
    "    edge_list = jnp.array(list(unique_edges))\n",
    "    senders = edge_list[:, 0]\n",
    "    receivers = edge_list[:, 1]\n",
    "\n",
    "    on_face_x0 = np.isclose(positions[:, 0], 0.0)\n",
    "    on_face_x1 = np.isclose(positions[:, 0], 1.0)\n",
    "    on_face_y0 = np.isclose(positions[:, 1], 0.0)\n",
    "    on_face_y1 = np.isclose(positions[:, 1], 1.0)\n",
    "    on_face_z0 = np.isclose(positions[:, 2], 0.0)\n",
    "    on_face_z1 = np.isclose(positions[:, 2], 1.0)\n",
    "\n",
    "    is_on_any_face = (on_face_x0 | on_face_x1 |\n",
    "                      on_face_y0 | on_face_y1 |\n",
    "                      on_face_z0 | on_face_z1)\n",
    "\n",
    "    boundary_nodes = np.where(is_on_any_face)[0]\n",
    "\n",
    "    print(\"Data extraction complete.\\n\")\n",
    "    print(f\"Positions array shape: {positions.shape}\")\n",
    "    print(f\"Boundary indices array shape: {boundary_nodes.shape}\")\n",
    "    print(f\"Senders array shape: {senders.shape}\")\n",
    "    print(f\"Receivers array shape: {receivers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202054",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8994d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unpickled data.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file = r\"/home/samuel/Github/Research-Placement/data/simulation_results.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(data_file, \"rb\") as f:\n",
    "        data_unpickled_1 = pickle.load(f)\n",
    "    print(f\"Successfully unpickled data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {data_file}\")\n",
    "    dataset_list = {}\n",
    "\n",
    "dataset_dict = data_unpickled_1\n",
    "\n",
    "# Not tunable, is known from how many sims ran\n",
    "num_sims = 10\n",
    "# permutation list for batching\n",
    "index_list = jnp.arange(num_sims)\n",
    "permutated_index_list = jax.random.permutation(jax.random.PRNGKey(0), index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ce7c1",
   "metadata": {},
   "source": [
    "check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d690e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1806, 3)\n",
      "(1331, 3)\n",
      "()\n",
      "(1806, 3)\n",
      "[[-2.9210228e-04 -6.3567061e-04 -1.4251155e-03]\n",
      " [-4.4415184e-04  8.9642766e-05 -6.0152575e-05]\n",
      " [-3.8378930e-04 -1.0108644e-04 -1.7809425e-05]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "[[-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " [-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " [-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " ...\n",
      " [ 1.5876233e-03 -1.5465567e-04 -2.3625712e-03]\n",
      " [ 1.4152135e-03 -4.2626876e-04 -3.6217444e-04]\n",
      " [ 2.9660708e-03  7.4987329e-05  4.9074343e-03]]\n",
      "0.004835612\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[0]['boundary_strain_energy_gradient'].shape)\n",
    "print(dataset_dict[0]['full_displacement_vector'].shape)\n",
    "print(dataset_dict[0]['strain_energy'].shape)\n",
    "print(dataset_dict[0]['applied_boundary_displacements'].shape)\n",
    "\n",
    "print(dataset_dict[0]['boundary_strain_energy_gradient'])\n",
    "print(dataset_dict[0]['full_displacement_vector'])\n",
    "print(dataset_dict[0]['strain_energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183f4bc",
   "metadata": {},
   "source": [
    "Pre-processing functions - Need to be changed when preprocessing is implemented to accomodate the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4c6ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data = data[:split_idx]\n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a8d00",
   "metadata": {},
   "source": [
    "Data pre-processing and graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd47f917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    }
   ],
   "source": [
    "# Pre-processing\n",
    "processed_dataset_dict = dataset_dict\n",
    "\n",
    "graphs_list = []\n",
    "displacements_list = []\n",
    "target_e_list = []\n",
    "target_e_prime_list = []\n",
    "boundary_displacements_list = []\n",
    "\n",
    "# Graph Building\n",
    "for i in tqdm(range(num_sims), leave=False):\n",
    "    U = processed_dataset_dict[i]['full_displacement_vector']\n",
    "    U = jnp.array(U)\n",
    "    graph = build_graphs(\n",
    "        senders, \n",
    "        receivers, \n",
    "        positions, \n",
    "        boundary_nodes, \n",
    "        U\n",
    "    )\n",
    "    graphs_list.append(graph)\n",
    "    displacements_list.append(U)\n",
    "    target_e = jnp.array(processed_dataset_dict[i]['strain_energy'])\n",
    "    target_e_list.append(target_e)\n",
    "    target_e_prime = jnp.array(processed_dataset_dict[i]['boundary_strain_energy_gradient'])\n",
    "    target_e_prime_list.append(target_e_prime)\n",
    "    bdd = jnp.array(processed_dataset_dict[i]['applied_boundary_displacements'])\n",
    "    boundary_displacements_list.append(bdd)\n",
    "\n",
    "\n",
    "dataset = {\n",
    "    'graphs_list': graphs_list,\n",
    "    'displacements': displacements_list,\n",
    "    'target_e': target_e_list,\n",
    "    'target_e_prime': target_e_prime_list,\n",
    "    'boundary_displacements': boundary_displacements_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eda85",
   "metadata": {},
   "source": [
    "Batching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686d79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_and_split_dataset(dataset_dict, batch_size, train_split, CV_split, test_split, permutated_index_list):\n",
    "    total_samples = permutated_index_list.shape[0]\n",
    "    idx_train_samples = int(train_split * total_samples) \n",
    "    idx_test_samples = idx_train_samples + int(test_split * total_samples) \n",
    "\n",
    "    train_idx = list(permutated_index_list[:idx_train_samples])\n",
    "    test_idx = list(permutated_index_list[idx_train_samples:idx_test_samples])\n",
    "    CV_idx = list(permutated_index_list[idx_test_samples:])\n",
    "\n",
    "    def batch_indices(idx):  \n",
    "        if not idx:\n",
    "            return\n",
    "        num_samples = len(idx)\n",
    "        num_batches = num_samples // batch_size\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            \n",
    "            graphs_in_batch = [dataset_dict['graphs_list'][i] for i in batch_idx]\n",
    "            displacements_batch = [dataset_dict['displacements'][i] for i in batch_idx]\n",
    "            e_batch = [dataset_dict['target_e'][i] for i in batch_idx]\n",
    "            e_prime_batch = [dataset_dict['target_e_prime'][i] for i in batch_idx]\n",
    "\n",
    "            batched_graphs = jraph.batch(graphs_in_batch)\n",
    "            batched_displacements = jnp.array(displacements_batch)\n",
    "            batched_e = jnp.array(e_batch)\n",
    "            batched_e_prime = jnp.array(e_prime_batch)\n",
    "\n",
    "            yield {\n",
    "                'graphs': batched_graphs, \n",
    "                'displacements': batched_displacements, \n",
    "                'target_e': batched_e, \n",
    "                'target_e_prime': batched_e_prime\n",
    "            }\n",
    "    \n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    CV_batches = list(batch_indices(CV_idx))\n",
    "\n",
    "    return train_batches, CV_batches, test_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9a939",
   "metadata": {},
   "source": [
    "Batching graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b07a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, CV_batches, test_batches = batch_and_split_dataset(\n",
    "    dataset, \n",
    "    batch_size, \n",
    "    train_split, \n",
    "    CV_split, \n",
    "    test_split, \n",
    "    permutated_index_list\n",
    ")\n",
    "\n",
    "is_known_zero = Get_known(boundary_nodes, positions)\n",
    "is_known_zero_expanded = jnp.expand_dims(is_known_zero, axis=1)\n",
    "U_zero = jnp.zeros_like(positions)\n",
    "node_features_zero = jnp.concatenate([positions, U_zero, is_known_zero_expanded], axis=1)\n",
    "\n",
    "zero_graph = jraph.GraphsTuple(\n",
    "    nodes=node_features_zero,\n",
    "    senders=senders,\n",
    "    receivers=receivers,\n",
    "    edges=None,\n",
    "    globals=None, \n",
    "    n_node=jnp.array([positions.shape[0]]),\n",
    "    n_edge=jnp.array([len(senders)])\n",
    ")\n",
    "\n",
    "zero_list = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    zero_list.append(zero_graph)\n",
    "\n",
    "batched_zero_graph = jraph.batch(zero_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c22883",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2633a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Silu(x: jax.Array) -> jax.Array:\n",
    "    return x * nnx.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615042f",
   "metadata": {},
   "source": [
    "GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1030d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GATEdges:\n",
    "    score: jax.Array\n",
    "    message: jax.Array\n",
    "\n",
    "class GAT(nnx.Module):\n",
    "    def __init__(self, in_features, out_features, *, rngs):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.GNN = jraph.GraphNetwork(\n",
    "            update_edge_fn=self.update_edge_fn,\n",
    "            update_node_fn=self.update_node_fn,\n",
    "            aggregate_edges_for_nodes_fn=self.aggregate_edges_for_nodes_fn\n",
    "        )\n",
    "\n",
    "        initialiser = nnx.initializers.lecun_normal()\n",
    "        self.Weight_mat = nnx.Param(initialiser(rngs.params(), (in_features, out_features)))\n",
    "        self.Attention_mat = nnx.Param(initialiser(rngs.params(), (2 * out_features, 1)))\n",
    "\n",
    "    def update_edge_fn(self, edges, senders, receivers, globals_):\n",
    "        \"computes edge features and outputs a dict containing the edge features\"\n",
    "        h_sender = senders @ self.Weight_mat\n",
    "        h_reciever = receivers @ self.Weight_mat\n",
    "\n",
    "        send_recieve_features = jnp.concatenate([h_sender, h_reciever], axis=-1)\n",
    "        attention_scores = nnx.leaky_relu(send_recieve_features @ self.Attention_mat)\n",
    "\n",
    "        return GATEdges(score=attention_scores, message=h_sender)\n",
    "    \n",
    "    def aggregate_edges_for_nodes_fn(self, edges: GATEdges, segment_ids, num_segments) -> jax.Array:\n",
    "        \"aggregates all edge messages for a node and outputs the aggregated messages\"\n",
    "        attention_coeffs = jraph.segment_softmax(\n",
    "            logits=edges.score,\n",
    "            segment_ids=segment_ids,\n",
    "            num_segments=num_segments\n",
    "        )\n",
    "\n",
    "        weighted_messages = edges.message * attention_coeffs\n",
    "\n",
    "        aggregated_messages = jraph.segment_sum(\n",
    "            data=weighted_messages,\n",
    "            segment_ids=segment_ids, \n",
    "            num_segments=num_segments\n",
    "        )\n",
    "\n",
    "        return aggregated_messages\n",
    "\n",
    "    def update_node_fn(self, nodes, sent_edges, received_edges, globals_):\n",
    "        \"takes aggregated node messages and applies them to the graph\"\n",
    "        return received_edges\n",
    "\n",
    "    def __call__(self, graph):\n",
    "        return self.GNN(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045c04d",
   "metadata": {},
   "source": [
    "SAGPool WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a377c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGPool(nnx.Module): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314af953",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nnx.Module):\n",
    "    def __init__(self, node_feature_dim: int, embedding_dim: int, output_dim: int, rngs: nnx.Rngs):\n",
    "        self.embedding_layer = nnx.Linear(node_feature_dim, embedding_dim, rngs=rngs)\n",
    "        self.encoderL1 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL1 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL2 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL2 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL3 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.decoding_layer = nnx.Linear(embedding_dim, output_dim, rngs=rngs)\n",
    "\n",
    "        self.node_graph_indices = None\n",
    "    \n",
    "    def embedder(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        embeddings = self.embedding_layer(nodes)\n",
    "        return graph._replace(nodes=embeddings)\n",
    "    \n",
    "    def apply_activation_and_res(self, graph: jraph.GraphsTuple, residual: jax.Array) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        activated_nodes = nnx.relu(nodes) + residual\n",
    "        return graph._replace(nodes=activated_nodes)\n",
    "    \n",
    "    def apply_res(self, graph: jraph.GraphsTuple, residual: jax.Array) -> jraph.GraphsTuple:\n",
    "        new_nodes = graph.nodes + residual\n",
    "        return graph._replace(nodes=new_nodes)\n",
    "    \n",
    "    def make_node_graph_indices(self, n_node: jnp.ndarray) -> jax.Array:\n",
    "        \"\"\"Create a mapping from nodes to graphs. Should be called outside jit.\"\"\"\n",
    "        num_graphs = int(n_node.shape[0])\n",
    "        node_graph_indices = jnp.repeat(jnp.arange(num_graphs), n_node)  \n",
    "        self.node_graph_indices = node_graph_indices\n",
    "        \n",
    "    def decoder(self, graph: jraph.GraphsTuple) -> jax.Array: # Switch to SAGPool when its finished\n",
    "        num_graphs = graph.n_node.shape[0]\n",
    "        aggregate_nodes = jraph.segment_sum(\n",
    "            data=graph.nodes, \n",
    "            segment_ids=self.node_graph_indices,\n",
    "            num_segments=num_graphs\n",
    "        )\n",
    "        return self.decoding_layer(aggregate_nodes)\n",
    "        \n",
    "    def forward_pass(self, G: jraph.GraphsTuple, use_running_average: bool) -> jax.Array:\n",
    "        G = self.embedder(G)\n",
    "        res1 = G.nodes\n",
    "\n",
    "        G = self.encoderL1(G)\n",
    "        nodes_norm = self.BatchNormL1(\n",
    "            G.nodes, \n",
    "            use_running_average=use_running_average\n",
    "        )\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res1)\n",
    "        res2 = G.nodes\n",
    "\n",
    "        G = self.encoderL2(G)\n",
    "        nodes_norm = self.BatchNormL2(\n",
    "            G.nodes,\n",
    "            use_running_average=use_running_average\n",
    "        )\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res2)\n",
    "        res3 = G.nodes\n",
    "\n",
    "        G = self.encoderL3(G)\n",
    "        G = self.apply_res(G, res3)\n",
    "\n",
    "        e = self.decoder(G)\n",
    "        return e\n",
    "    \n",
    "    def __call__(self, G: jraph.GraphsTuple, use_running_average: bool):\n",
    "\n",
    "        e = self.forward_pass(G, use_running_average)\n",
    "\n",
    "        def energy_fn(nodes):\n",
    "            G_temp = G._replace(nodes=nodes)\n",
    "            return self.forward_pass(G_temp, use_running_average=True)\n",
    "\n",
    "        grad_fn = jax.grad(energy_fn)\n",
    "        grads = grad_fn(G.nodes)\n",
    "\n",
    "        e_prime = grads[:, 3:6]\n",
    "\n",
    "        return e, e_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9615288",
   "metadata": {},
   "source": [
    "Loss function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(batch, batched_zero_graph,*, Model, use_running_average, alpha, gamma, lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    target_e_batch = batch['target_e']\n",
    "    target_e_prime_batch = batch['target_e_prime']\n",
    "    graph_batch = batch['graphs']\n",
    "    \n",
    "    prediction_e, prediction_e_prime = Model(graph_batch, use_running_average)\n",
    "    loss_e = jnp.mean((prediction_e - target_e_batch)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime_batch)**2)\n",
    "    \n",
    "    prediction_zero, _ = Model(batched_zero_graph, use_running_average=False)\n",
    "    loss_zero = jnp.mean((prediction_zero - 0.0)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131db7c5",
   "metadata": {},
   "source": [
    "CV loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_loss_fn(CV_batches, batched_zero_graph, Model: GNN, alpha, gamma, lambda_):\n",
    "    CV_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    Model.eval()\n",
    "    for CV_batch in CV_batches:\n",
    "        Model.make_node_graph_indices(CV_batch['graphs'].n_node)\n",
    "        batch_count += 1\n",
    "\n",
    "        loss = loss_fn(\n",
    "            CV_batch,\n",
    "            batched_zero_graph,\n",
    "            Model=Model,\n",
    "            use_running_average=True,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        CV_loss += loss\n",
    "\n",
    "    if batch_count > 0:\n",
    "        CV_loss = CV_loss / batch_count\n",
    "        return CV_loss\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24c632",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def train_step(Model, optimiser, GraphandTarget_batch, batched_zero_graph, *, alpha, gamma, lambda_):\n",
    "\n",
    "    def wrapped_loss(Model):\n",
    "        loss = loss_fn(\n",
    "            GraphandTarget_batch,\n",
    "            batched_zero_graph,\n",
    "            Model=Model,\n",
    "            use_running_average=False,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_ \n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss, argnums=0)(Model)\n",
    "    optimiser.update(grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98768043",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59280fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: traced array with shape int32[]\nIt arose in the jnp.arange argument 'stop'\nThe error occurred while tracing the function train_step at /tmp/ipykernel_2025400/560498724.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument GraphandTarget_batch['graphs'].n_node.\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConcretizationTypeError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_batches, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     31\u001b[39m     Model.train()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     batch_loss = train_step(\n\u001b[32m     33\u001b[39m         Model,\n\u001b[32m     34\u001b[39m         optimiser,\n\u001b[32m     35\u001b[39m         batch,\n\u001b[32m     36\u001b[39m         batched_zero_graph,\n\u001b[32m     37\u001b[39m         alpha=alpha,\n\u001b[32m     38\u001b[39m         gamma=gamma,\n\u001b[32m     39\u001b[39m         lambda_=lambda_\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     42\u001b[39m     batch_count += \u001b[32m1\u001b[39m\n\u001b[32m     43\u001b[39m     running_loss += batch_loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/flax/nnx/transforms/compilation.py:431\u001b[39m, in \u001b[36mJitWrapped.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    430\u001b[39m   pure_args, pure_kwargs = \u001b[38;5;28mself\u001b[39m._get_pure_args_kwargs(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[38;5;28mself\u001b[39m.jitted_fn(\n\u001b[32m    432\u001b[39m     *pure_args, **pure_kwargs\n\u001b[32m    433\u001b[39m   )\n\u001b[32m    434\u001b[39m   out = \u001b[38;5;28mself\u001b[39m._get_non_pure_out(pure_args_out, pure_kwargs_out, pure_out)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/flax/nnx/transforms/compilation.py:126\u001b[39m, in \u001b[36mJitFn.__call__\u001b[39m\u001b[34m(self, *pure_args, **pure_kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *pure_args, **pure_kwargs):\n\u001b[32m    119\u001b[39m   args, kwargs = extract.from_tree(\n\u001b[32m    120\u001b[39m     (pure_args, pure_kwargs),\n\u001b[32m    121\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    122\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    123\u001b[39m     is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    124\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m   out = \u001b[38;5;28mself\u001b[39m.f(*args, **kwargs)\n\u001b[32m    128\u001b[39m   args_out, kwargs_out = extract.clear_non_graph_nodes((args, kwargs))\n\u001b[32m    129\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = extract.to_tree(\n\u001b[32m    130\u001b[39m     (args_out, kwargs_out, out),\n\u001b[32m    131\u001b[39m     prefix=(\u001b[38;5;28mself\u001b[39m.in_shardings, \u001b[38;5;28mself\u001b[39m.kwarg_shardings, \u001b[38;5;28mself\u001b[39m.out_shardings),\n\u001b[32m    132\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    133\u001b[39m     split_fn=_jit_split_fn,\n\u001b[32m    134\u001b[39m   )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(Model, optimiser, GraphandTarget_batch, batched_zero_graph, alpha, gamma, lambda_)\u001b[39m\n\u001b[32m      5\u001b[39m     loss = loss_fn(\n\u001b[32m      6\u001b[39m         GraphandTarget_batch,\n\u001b[32m      7\u001b[39m         batched_zero_graph,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m         lam=lambda_ \n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m loss, grads = nnx.value_and_grad(wrapped_loss, argnums=\u001b[32m0\u001b[39m)(Model)\n\u001b[32m     16\u001b[39m optimiser.update(grads)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/flax/nnx/graph.py:2049\u001b[39m, in \u001b[36mUpdateContextManager.__call__.<locals>.update_context_manager_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m   2047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_context_manager_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m   2048\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/flax/nnx/transforms/autodiff.py:163\u001b[39m, in \u001b[36m_grad_general.<locals>.grad_wrapper\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    151\u001b[39m pure_args = extract.to_tree(\n\u001b[32m    152\u001b[39m   args, prefix=arg_filters, split_fn=_grad_split_fn, ctxtag=\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m gradded_fn = transform(\n\u001b[32m    156\u001b[39m   GradFn(f, has_aux, nondiff_states),\n\u001b[32m    157\u001b[39m   argnums=jax_argnums,\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m   allow_int=allow_int,\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m fn_out = gradded_fn(*pure_args)\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_grads\u001b[39m(grads):\n\u001b[32m    166\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m jax.tree.map(\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x.state \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, extract.NodeStates) \u001b[38;5;28;01melse\u001b[39;00m x,\n\u001b[32m    168\u001b[39m     grads,\n\u001b[32m    169\u001b[39m     is_leaf=\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28misinstance\u001b[39m(x, extract.NodeStates),\n\u001b[32m    170\u001b[39m   )\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/flax/nnx/transforms/autodiff.py:88\u001b[39m, in \u001b[36mGradFn.__call__\u001b[39m\u001b[34m(self, *pure_args)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ctx.merge(value.graphdef, value.state, nondiff)\n\u001b[32m     84\u001b[39m args = extract.from_tree(\n\u001b[32m     85\u001b[39m   pure_args, merge_fn=_grad_merge_fn, ctxtag=\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m, is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     86\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m out = \u001b[38;5;28mself\u001b[39m.f(*args)\n\u001b[32m     90\u001b[39m args_out = extract.clear_non_graph_nodes(args)\n\u001b[32m     91\u001b[39m pure_args_out, pure_out = extract.to_tree((args_out, out), ctxtag=\u001b[33m'\u001b[39m\u001b[33mgrad\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[108]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mtrain_step.<locals>.wrapped_loss\u001b[39m\u001b[34m(Model)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_loss\u001b[39m(Model):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     loss = loss_fn(\n\u001b[32m      6\u001b[39m         GraphandTarget_batch,\n\u001b[32m      7\u001b[39m         batched_zero_graph,\n\u001b[32m      8\u001b[39m         Model=Model,\n\u001b[32m      9\u001b[39m         alpha=alpha,\n\u001b[32m     10\u001b[39m         gamma=gamma,\n\u001b[32m     11\u001b[39m         lam=lambda_ \n\u001b[32m     12\u001b[39m     )\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mloss_fn\u001b[39m\u001b[34m(batch, batched_zero_graph, Model, alpha, gamma, lam)\u001b[39m\n\u001b[32m      8\u001b[39m target_e_prime_batch = batch[\u001b[33m'\u001b[39m\u001b[33mtarget_e_prime\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      9\u001b[39m graph_batch = batch[\u001b[33m'\u001b[39m\u001b[33mgraphs\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m prediction_e, prediction_e_prime = Model(graph_batch, use_running_average=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m loss_e = jnp.mean((prediction_e - target_e_batch)**\u001b[32m2\u001b[39m)\n\u001b[32m     13\u001b[39m loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime_batch)**\u001b[32m2\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mGNN.__call__\u001b[39m\u001b[34m(self, G, use_running_average)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, G: jraph.GraphsTuple, use_running_average):\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     e = \u001b[38;5;28mself\u001b[39m.forward_pass(G, use_running_average)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdifferentiable_forward\u001b[39m(nodes):\n\u001b[32m     64\u001b[39m         G_temp = G._replace(nodes=nodes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mGNN.forward_pass\u001b[39m\u001b[34m(self, G, use_running_average)\u001b[39m\n\u001b[32m     53\u001b[39m G = \u001b[38;5;28mself\u001b[39m.encoderL3(G)\n\u001b[32m     54\u001b[39m G = \u001b[38;5;28mself\u001b[39m.apply_res(G, res3)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m e = \u001b[38;5;28mself\u001b[39m.decoder(G)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mGNN.decoder\u001b[39m\u001b[34m(self, graph)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph: jraph.GraphsTuple) -> jraph.GraphsTuple: \u001b[38;5;66;03m# Switch to SAGPool when its finished\u001b[39;00m\n\u001b[32m     26\u001b[39m     num_graphs = graph.n_node.shape[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     node_graph_indices = get_graph_indices(graph.n_node)\n\u001b[32m     28\u001b[39m     aggregate_nodes = jax.ops.segment_sum(\n\u001b[32m     29\u001b[39m         data=graph.nodes, \n\u001b[32m     30\u001b[39m         segment_ids=node_graph_indices,\n\u001b[32m     31\u001b[39m         num_segments=num_graphs\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoding_layer(aggregate_nodes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mget_graph_indices\u001b[39m\u001b[34m(n_node)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_graph_indices\u001b[39m(n_node: jnp.ndarray) -> jnp.ndarray:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp.arange(n_node.sum()) - jnp.repeat(jnp.cumsum(n_node) - n_node, n_node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5966\u001b[39m, in \u001b[36marange\u001b[39m\u001b[34m(start, stop, step, dtype, device, out_sharding)\u001b[39m\n\u001b[32m   5964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sharding._is_concrete:\n\u001b[32m   5965\u001b[39m   \u001b[38;5;28;01massert\u001b[39;00m sharding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sharding, NamedSharding)\n\u001b[32m-> \u001b[39m\u001b[32m5966\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _arange(start, stop=stop, step=step, dtype=dtype,\n\u001b[32m   5967\u001b[39m                  out_sharding=sharding)\n\u001b[32m   5968\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5969\u001b[39m   output = _arange(start, stop=stop, step=step, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:5980\u001b[39m, in \u001b[36m_arange\u001b[39m\u001b[34m(start, stop, step, dtype, out_sharding)\u001b[39m\n\u001b[32m   5978\u001b[39m util.check_arraylike(\u001b[33m\"\u001b[39m\u001b[33marange\u001b[39m\u001b[33m\"\u001b[39m, start)\n\u001b[32m   5979\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5980\u001b[39m   start = core.concrete_or_error(\u001b[38;5;28;01mNone\u001b[39;00m, start, \u001b[33m\"\u001b[39m\u001b[33mIt arose in the jnp.arange argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5981\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5982\u001b[39m   start = core.concrete_or_error(\u001b[38;5;28;01mNone\u001b[39;00m, start, \u001b[33m\"\u001b[39m\u001b[33mIt arose in the jnp.arange argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/core.py:1737\u001b[39m, in \u001b[36mconcrete_or_error\u001b[39m\u001b[34m(force, val, context)\u001b[39m\n\u001b[32m   1735\u001b[39m maybe_concrete = val.to_concrete_value()\n\u001b[32m   1736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maybe_concrete \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(val, context)\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1739\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m force(maybe_concrete)\n",
      "\u001b[31mConcretizationTypeError\u001b[39m: Abstract tracer value encountered where concrete value is expected: traced array with shape int32[]\nIt arose in the jnp.arange argument 'stop'\nThe error occurred while tracing the function train_step at /tmp/ipykernel_2025400/560498724.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument GraphandTarget_batch['graphs'].n_node.\n\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "Model = GNN(\n",
    "    node_feature_dim=7, \n",
    "    embedding_dim=128,\n",
    "    output_dim=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "optimiser = nnx.Optimizer(\n",
    "    Model,\n",
    "    optax.adam(\n",
    "        learning_rate=Learn_Rate, \n",
    "        b1=beta_1, \n",
    "        b2=beta_2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "loss_record = []\n",
    "CV_loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for batch in tqdm(train_batches, desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "        Model.make_node_graph_indices(batch['graphs'].n_node)\n",
    "        Model.train()\n",
    "        batch_loss = train_step(\n",
    "            Model,\n",
    "            optimiser,\n",
    "            batch,\n",
    "            batched_zero_graph,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        batch_count += 1\n",
    "        running_loss += batch_loss\n",
    "\n",
    "    CV_loss = CV_loss_fn(\n",
    "        CV_batches,\n",
    "        batched_zero_graph,\n",
    "        Model,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        lambda_\n",
    "    )\n",
    "    \n",
    "    loss_record.append(running_loss)\n",
    "    CV_loss_record.append(CV_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402f7aa",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7d07b3a57890>]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_WSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
