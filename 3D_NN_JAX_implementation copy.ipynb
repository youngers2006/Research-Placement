{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax import nnx\n",
    "from flax import struct\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc34597",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cae85666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 1000\n",
    "alpha = 1.0\n",
    "gamma = 0.4\n",
    "lambda_ = 0.1\n",
    "Learn_Rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "Batch_size = 40\n",
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "002646d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2340, 152, 3)\n",
      "(10000, 152, 3)\n",
      "(10000,)\n",
      "(10000, 152, 3)\n",
      "(12340, 152, 3)\n",
      "(12340, 1)\n",
      "(12340, 152, 3)\n"
     ]
    }
   ],
   "source": [
    "# Due to errors I was experiencing this seems to be the quickest fix I could find to allow me to unpickle the data\n",
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file_1 = r\"C:\\Users\\samue\\Downloads\\Simulation.pickle\"\n",
    "data_file_2 = r\"C:\\Users\\samue\\Downloads\\Simulation 2.pickle\"\n",
    "\n",
    "with open(data_file_1,\"rb\") as f:\n",
    "    data_unpickled_1 = pickle.load(f)\n",
    "\n",
    "with open(data_file_2,\"rb\") as f:\n",
    "    data_unpickled_2 = pickle.load(f)\n",
    "\n",
    "_,data_object_1 = data_unpickled_1\n",
    "_,data_object_2 = data_unpickled_2\n",
    "\n",
    "input_dataset_1 = jnp.array(data_object_1.Indata)\n",
    "#data_index_1 = data_object_1.i\n",
    "e_dataset_1 = jnp.array(data_object_1.SE)\n",
    "e_prime_dataset_1 = jnp.array(data_object_1.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)\n",
    "#data_index_2 = data_object_2.i\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)[0:2340]\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)[0:2340]\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)[0:2340]\n",
    "\n",
    "print(input_dataset_2.shape)\n",
    "print(input_dataset_1.shape)\n",
    "print(e_dataset_1.shape)\n",
    "print(e_prime_dataset_1.shape)\n",
    "\n",
    "input_dataset = jax.numpy.concatenate([input_dataset_1,input_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.concatenate([e_dataset_1, e_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.expand_dims(target_e_dataset,axis=1)\n",
    "target_e_prime_dataset = jax.numpy.concatenate([e_prime_dataset_1,e_prime_dataset_2],axis=0)\n",
    "\n",
    "print(input_dataset.shape)\n",
    "print(target_e_dataset.shape)\n",
    "print(target_e_prime_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5ebb",
   "metadata": {},
   "source": [
    "Redimensionalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redimensionalise(self):\n",
    "    self.Disp = jnp.zeros((self.Dims,self.Dims,self.Dims,3))\n",
    "    m = 0\n",
    "    for i in range(self.Dims):\n",
    "        for j in range(self.Dims):\n",
    "            for k in range(self.Dims):\n",
    "                if self.xInMesh[0][i,j,k] == 0 or self.xInMesh[0][i,j,k] == 1 or self.xInMesh[1][i,j,k] == 0 or self.xInMesh[1][i,j,k] == 1 or self.xInMesh[2][i,j,k] == 0 or self.xInMesh[2][i,j,k] == 1:\n",
    "                    self.Disp[i,j,k,:] = self.RandDisp[self.Index,m,:]\n",
    "                    m = m +1\n",
    "    return self.Disp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53d879",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "debbce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec14bee",
   "metadata": {},
   "source": [
    "Pre and post processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6607d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data = data[:split_idx]\n",
    "    \n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def mean_and_std_dev_square_feature(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data_init = data[:split_idx]\n",
    "\n",
    "    additional_feature = jnp.square(train_data_init)\n",
    "\n",
    "    train_data = jnp.concatenate([train_data_init,additional_feature],axis=1)\n",
    "    \n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "\n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328deec4",
   "metadata": {},
   "source": [
    "Dataset - Note: need to remove input scaling in the dataset as its done in the model, need to add a parameter calulator for the input that concatenates the square data then gets params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e4221141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTING RAW DATASET\n",
      "Key: 'displacements'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340,)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e_prime'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_num = input_dataset.shape[0] // Batch_size\n",
    "\n",
    "input_dataset = input_dataset.reshape((input_dataset.shape[0],456))\n",
    "target_e_dataset = target_e_dataset.reshape((target_e_dataset.shape[0],))\n",
    "target_e_prime_dataset = target_e_prime_dataset.reshape((target_e_prime_dataset.shape[0],456))\n",
    "\n",
    "params_dict_displacement = mean_and_std_dev_square_feature(input_dataset,train_split=train_split)\n",
    "params_dict_target_e = mean_and_std_dev(target_e_dataset,train_split=train_split)\n",
    "params_dict_target_e_prime = mean_and_std_dev(target_e_prime_dataset,train_split=train_split)\n",
    "\n",
    "target_e_dataset_scaled = scale_data(target_e_dataset, data_params=params_dict_target_e)\n",
    "target_e_prime_dataset_scaled = scale_data(target_e_prime_dataset, data_params=params_dict_target_e_prime)\n",
    "\n",
    "Dataset_parameters = {\n",
    "    'displacements':params_dict_displacement,\n",
    "    'target_e':params_dict_target_e,\n",
    "    'target_e_prime':params_dict_target_e_prime\n",
    "}\n",
    "\n",
    "Dataset = {\n",
    "    'displacements':input_dataset, # Dataset not scaled as its scaled in the model\n",
    "    'target_e':target_e_dataset_scaled,\n",
    "    'target_e_prime':target_e_prime_dataset_scaled\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"INSPECTING RAW DATASET\")\n",
    "for key, value in Dataset.items():\n",
    "    print(f\"Key: '{key}'\")\n",
    "    print(f\"  - Type: {type(value)}\")\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  - Shape: {value.shape}\")\n",
    "    else:\n",
    "        print(\"  - No shape attribute.\")\n",
    "    if hasattr(value, 'dtype'):\n",
    "        print(f\"  - Dtype: {value.dtype}\")\n",
    "print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b799cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0034145229\n",
      "2.932326\n",
      "4.1916647\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['displacements'][0][1])\n",
    "print(Dataset['target_e'][0])\n",
    "print(Dataset['target_e_prime'][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943c7c2",
   "metadata": {},
   "source": [
    "Node Classes and Acivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8a77bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"Linear node for neural network\"\"\"\n",
    "\n",
    "    def __init__(self,din: int,dout: int,*,rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W = nnx.Param(jax.random.uniform(key=key, shape=(din,dout)))\n",
    "        self.b = nnx.Param(jnp.zeros(shape=(dout,)))\n",
    "        self.din, self.dout = din, dout\n",
    "\n",
    "    def __call__(self,x: jax.Array):\n",
    "        return(x @ self.W + self.b)\n",
    "    \n",
    "def SiLU(x: jax.Array):\n",
    "    \"\"\"Sigmoid Weighted Linear Unit activation function\"\"\"\n",
    "    return x * jax.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26f9bc",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59bfbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class energy_prediction(nnx.Module):\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "\n",
    "    def __init__(self,dim_in: int, dim_hidden1_in: int, dim_hidden2_in: int,dim_hidden3_in,dim_out: int,*,rngs: nnx.Rngs):\n",
    "        self.layer1 = Linear(din=dim_in,dout=dim_hidden1_in,rngs=rngs)\n",
    "        self.layer2 = Linear(din=dim_hidden1_in,dout=dim_hidden2_in,rngs=rngs)\n",
    "        self.layer3 = Linear(din=dim_hidden2_in,dout=dim_hidden3_in,rngs=rngs)\n",
    "        self.output_layer = Linear(din=dim_hidden3_in,dout=dim_out,rngs=rngs)\n",
    "        self.silu = SiLU\n",
    "        \n",
    "    def __call__(self,x_in,data_params):\n",
    "        # pass to calculate e\n",
    "        def forwardPass(x_init,data_params):\n",
    "            x_ft = jnp.square(x_init)\n",
    "            x_combined = jnp.concatenate([x_init, x_ft])\n",
    "\n",
    "            def scale_data(data,*, data_params):\n",
    "                return (data - data_params['mean']) / data_params['std_dev']\n",
    "            \n",
    "            x = scale_data(x_combined,data_params=data_params)\n",
    "\n",
    "            x = self.layer1(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.output_layer(x)\n",
    "            return x.squeeze()\n",
    "        \n",
    "        e = jax.vmap(forwardPass, in_axes=(0, None))(x_in, data_params)\n",
    "        dedx = jax.vmap(jax.grad(forwardPass,argnums=(0)), in_axes=(0, None))\n",
    "        e_prime = dedx(x_in, data_params)\n",
    "\n",
    "        return e, e_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc036a78",
   "metadata": {},
   "source": [
    "Define optimiser and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f469e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=Learn_Rate, b1=beta_1, b2=beta_2)\n",
    "\n",
    "def loss_fn(x: jax.Array, target_e, target_e_prime,*, Model, Dataset_parameters, alpha, gamma, lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    data_params = Dataset_parameters['displacements']\n",
    "    prediction_e, prediction_e_prime = Model(x, data_params)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime)**2)\n",
    "\n",
    "    target_zero = 0\n",
    "    x_zero = jnp.zeros(x[0].shape)\n",
    "    x_zero = jnp.expand_dims(x_zero, axis=0)\n",
    "    prediction_zero, _ = Model(x_zero, data_params)\n",
    "    loss_zero = jnp.mean((prediction_zero - target_zero)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57659f0",
   "metadata": {},
   "source": [
    "Train State Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b764c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class TrainState(nnx.Object):\n",
    "    params: Any\n",
    "    graph_def: Any \n",
    "    state: Any\n",
    "    alpha: float \n",
    "    gamma: float \n",
    "    lambda_: float "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118860",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "da6228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def training_step(params,state,opt_state,batch,*,graph_def,Dataset_parameters,alpha,gamma,lambda_):\n",
    "\n",
    "    disp_in = batch['displacements']\n",
    "    e_target = batch['target_e']\n",
    "    e_prime_target = batch['target_e_prime']\n",
    "\n",
    "    def wrapped_loss_fn(params_,state_):\n",
    "        Model = nnx.merge(graph_def,params_,state_)\n",
    "        loss = loss_fn(\n",
    "            disp_in,\n",
    "            e_target,\n",
    "            e_prime_target,\n",
    "            Model=Model,\n",
    "            Dataset_parameters=Dataset_parameters,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss_fn, argnums=0)(params, state) \n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_state = state\n",
    "\n",
    "    return new_params, new_state, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88cb1",
   "metadata": {},
   "source": [
    "Batch Creator and test set creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b9981b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_batch_dataset(dataset, batch_size, test_split=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets, then yields batches for each.\n",
    "    Returns: (train_batches, test_batches).\n",
    "    \"\"\"\n",
    "    N = dataset['displacements'].shape[0]\n",
    "    indices = jnp.arange(N)\n",
    "    if shuffle:\n",
    "        indices = jax.random.permutation(jax.random.PRNGKey(0), indices)\n",
    "    split_idx = int(N * (1 - test_split))\n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "\n",
    "    def batch_indices(idx):\n",
    "        batch_num = len(idx) // batch_size\n",
    "        for i in range(batch_num):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            batch = {key: value[batch_idx] for key, value in dataset.items()}\n",
    "            yield batch\n",
    "\n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e467e7e",
   "metadata": {},
   "source": [
    "Create test and train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df10eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, test_batches = split_and_batch_dataset(\n",
    "    Dataset, \n",
    "    Batch_size, \n",
    "    test_split=(1 - train_split), \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df560003",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "23fdd255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = energy_prediction(\n",
    "    dim_in=(input_dataset.shape[1] * 2), \n",
    "    dim_hidden1_in=1024,\n",
    "    dim_hidden2_in=512,\n",
    "    dim_hidden3_in=256, \n",
    "    dim_out=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "graph_def,params,state = nnx.split(Model,nnx.Param,nnx.State)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "train_state = TrainState(\n",
    "    graph_def=graph_def,\n",
    "    params=params,\n",
    "    state=state,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    lambda_=lambda_\n",
    "    )\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(train_batches,desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "        \n",
    "        new_params, new_state, new_opt_state, loss_batch = training_step(\n",
    "            train_state.params,\n",
    "            train_state.state,\n",
    "            opt_state,\n",
    "            batch,\n",
    "            graph_def=train_state.graph_def,\n",
    "            Dataset_parameters=Dataset_parameters,\n",
    "            alpha=train_state.alpha,\n",
    "            gamma=train_state.gamma,\n",
    "            lambda_=train_state.lambda_\n",
    "        )\n",
    "\n",
    "        opt_state = new_opt_state\n",
    "        train_state.params = new_params\n",
    "        train_state.state = new_state\n",
    "\n",
    "        running_loss += loss_batch\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = avg_loss = running_loss / batch_count if batch_count > 0 else 0.0\n",
    "    loss_record.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e850b4",
   "metadata": {},
   "source": [
    "Final model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0738a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class ModelData(nnx.Object):\n",
    "    graph_def: Any\n",
    "    params: Any\n",
    "    state: Any\n",
    "    trained: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62b354",
   "metadata": {},
   "source": [
    "Create Final model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78303138",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_def_trained = train_state.graph_def\n",
    "params_trained = train_state.params\n",
    "state_trained = train_state.state\n",
    "\n",
    "model_data = ModelData(\n",
    "    graph_def=graph_def_trained,\n",
    "    params=params_trained,\n",
    "    state = state_trained,\n",
    "    trained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc294f4",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "80f93141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b34507d950>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jnp.log10(jnp.array(loss_record)))\n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baeb33",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1645b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_abs_error(pred,target):\n",
    "    n1 = pred.shape[0]\n",
    "    n2 = target.shape[0]\n",
    "\n",
    "    if n1 != n2:\n",
    "        raise(\"Error: inputs must have matching shape\")\n",
    "    \n",
    "    return (jnp.sum(jnp.abs(pred - target)) / n1)\n",
    "\n",
    "def test_model(model_data, test_batches, Dataset_parameters,*,loss_fn, alpha, gamma, lambda_):\n",
    "\n",
    "    trained = model_data.trained\n",
    "    if not trained:\n",
    "        raise TypeError(\"Model is untrained, please train the model before evaluation\")\n",
    "\n",
    "    test_graph_def = model_data.graph_def\n",
    "    test_params = model_data.params\n",
    "    test_state = model_data.state\n",
    "\n",
    "    test_model = nnx.merge(test_graph_def,test_params,test_state)\n",
    "\n",
    "    loss_test = 0.0\n",
    "    test_count = 0\n",
    "\n",
    "    for batch in test_batches:\n",
    "        displacements_test = batch['displacements']\n",
    "        e_target_test = batch['target_e']\n",
    "        e_prime_target_test = batch['target_e_prime']\n",
    "\n",
    "        e_target_test = unscale_data(e_target_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_target_test = unscale_data(e_prime_target_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        e_pred_test, e_prime_pred_test = test_model(displacements_test,data_params=Dataset_parameters['displacements'])\n",
    "\n",
    "        #displacements_test = unscale_data(displacements_test,data_params=Dataset_parameters['displacements'])\n",
    "        e_pred_test = unscale_data(e_pred_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_pred_test = unscale_data(e_prime_pred_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        batch_loss_test = loss_fn(\n",
    "            displacements_test,\n",
    "            e_target_test,\n",
    "            e_prime_target_test,\n",
    "            Model=test_model,\n",
    "            Dataset_parameters=Dataset_parameters,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        loss_test += batch_loss_test\n",
    "        test_count += 1\n",
    "\n",
    "        avg_e_abs_error = avg_abs_error(e_pred_test,e_target_test)\n",
    "        avg_e_prime_abs_error = avg_abs_error(e_prime_pred_test,e_prime_target_test)\n",
    "\n",
    "    avg_loss_test = loss_test / test_count\n",
    "    zero_val_e, _ = test_model(jnp.zeros_like(test_batches[0]['displacements']), data_params=Dataset_parameters['target_e'])\n",
    "    test_e_zero_error = avg_abs_error(zero_val_e, jnp.zeros_like(zero_val_e))\n",
    "\n",
    "    return avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe3504",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1486d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average absolute error for e is 105.45194244384766 in the test set\n",
      "the average absolute error for e prime is 7588769.5 in the test set\n",
      "the absolute zero error for e is 0.1779184341430664 in the test set\n",
      "The average loss across the training set is 248372672.0\n",
      "The average absolute error for e is 73.6728744506836 in the training set\n",
      "the average absolute error for e prime is 3000363.5 in the training set\n",
      "the absolute zero error for e is 0.1779184341430664 in the training set\n"
     ]
    }
   ],
   "source": [
    "avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error = test_model(model_data,test_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    "avg_loss_training, avg_e_abs_error_training, avg_e_prime_abs_error_training, test_e_zero_error_training = test_model(model_data,train_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    " \n",
    "print(f\"The average absolute error for e is {avg_e_abs_error} in the test set\") \n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error} in the test set\") \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error} in the test set\") \n",
    "\n",
    "print(f\"The average loss across the training set is {avg_loss_test}\")\n",
    "print(f\"The average absolute error for e is {avg_e_abs_error_training} in the training set\")\n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error_training} in the training set\")  \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error_training} in the training set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_env_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
