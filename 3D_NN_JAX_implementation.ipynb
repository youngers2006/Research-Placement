{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax import nnx\n",
    "from flax import struct\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc34597",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae85666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 1000\n",
    "alpha = 1.0\n",
    "gamma = 0.4\n",
    "lambda_ = 0.1\n",
    "Learn_Rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "Batch_size = 40\n",
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002646d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2340, 152, 3)\n",
      "(10000, 152, 3)\n",
      "(10000,)\n",
      "(10000, 152, 3)\n",
      "(12340, 152, 3)\n",
      "(12340, 1)\n",
      "(12340, 152, 3)\n"
     ]
    }
   ],
   "source": [
    "# Due to errors I was experiencing this seems to be the quickest fix I could find to allow me to unpickle the data\n",
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file_1 = r\"C:\\Users\\samue\\Downloads\\Simulation.pickle\"\n",
    "data_file_2 = r\"C:\\Users\\samue\\Downloads\\Simulation 2.pickle\"\n",
    "\n",
    "with open(data_file_1,\"rb\") as f:\n",
    "    data_unpickled_1 = pickle.load(f)\n",
    "\n",
    "with open(data_file_2,\"rb\") as f:\n",
    "    data_unpickled_2 = pickle.load(f)\n",
    "\n",
    "_,data_object_1 = data_unpickled_1\n",
    "_,data_object_2 = data_unpickled_2\n",
    "\n",
    "input_dataset_1 = jnp.array(data_object_1.Indata)\n",
    "#data_index_1 = data_object_1.i\n",
    "e_dataset_1 = jnp.array(data_object_1.SE)\n",
    "e_prime_dataset_1 = jnp.array(data_object_1.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)\n",
    "#data_index_2 = data_object_2.i\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)[0:2340]\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)[0:2340]\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)[0:2340]\n",
    "\n",
    "print(input_dataset_2.shape)\n",
    "print(input_dataset_1.shape)\n",
    "print(e_dataset_1.shape)\n",
    "print(e_prime_dataset_1.shape)\n",
    "\n",
    "input_dataset = jax.numpy.concatenate([input_dataset_1,input_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.concatenate([e_dataset_1, e_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.expand_dims(target_e_dataset,axis=1)\n",
    "target_e_prime_dataset = jax.numpy.concatenate([e_prime_dataset_1,e_prime_dataset_2],axis=0)\n",
    "\n",
    "print(input_dataset.shape)\n",
    "print(target_e_dataset.shape)\n",
    "print(target_e_prime_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5ebb",
   "metadata": {},
   "source": [
    "Redimensionalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redimensionalise(self):\n",
    "    self.Disp = jnp.zeros((self.Dims,self.Dims,self.Dims,3))\n",
    "    m = 0\n",
    "    for i in range(self.Dims):\n",
    "        for j in range(self.Dims):\n",
    "            for k in range(self.Dims):\n",
    "                if self.xInMesh[0][i,j,k] == 0 or self.xInMesh[0][i,j,k] == 1 or self.xInMesh[1][i,j,k] == 0 or self.xInMesh[1][i,j,k] == 1 or self.xInMesh[2][i,j,k] == 0 or self.xInMesh[2][i,j,k] == 1:\n",
    "                    self.Disp[i,j,k,:] = self.RandDisp[self.Index,m,:]\n",
    "                    m = m +1\n",
    "    return self.Disp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53d879",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debbce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec14bee",
   "metadata": {},
   "source": [
    "Pre and post processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6607d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data = data[:split_idx]\n",
    "    \n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "\n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328deec4",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4221141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTING RAW DATASET\n",
      "Key: 'displacements'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340,)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e_prime'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_num = input_dataset.shape[0] // Batch_size\n",
    "\n",
    "input_dataset = input_dataset.reshape((input_dataset.shape[0],456))\n",
    "target_e_dataset = target_e_dataset.reshape((target_e_dataset.shape[0],))\n",
    "target_e_prime_dataset = target_e_prime_dataset.reshape((target_e_prime_dataset.shape[0],456))\n",
    "\n",
    "params_dict_displacement = mean_and_std_dev(input_dataset,train_split=train_split)\n",
    "params_dict_target_e = mean_and_std_dev(target_e_dataset,train_split=train_split)\n",
    "params_dict_target_e_prime = mean_and_std_dev(target_e_prime_dataset,train_split=train_split)\n",
    "\n",
    "input_dataset_scaled = scale_data(input_dataset, data_params=params_dict_displacement)\n",
    "target_e_dataset_scaled = scale_data(target_e_dataset, data_params=params_dict_target_e)\n",
    "target_e_prime_dataset_scaled = scale_data(target_e_prime_dataset, data_params=params_dict_target_e_prime)\n",
    "\n",
    "Dataset_parameters = {\n",
    "    'displacements':params_dict_displacement,\n",
    "    'target_e':params_dict_target_e,\n",
    "    'target_e_prime':params_dict_target_e_prime\n",
    "}\n",
    "\n",
    "Dataset = {\n",
    "    'displacements':input_dataset_scaled,\n",
    "    'target_e':target_e_dataset_scaled,\n",
    "    'target_e_prime':target_e_prime_dataset_scaled\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"INSPECTING RAW DATASET\")\n",
    "for key, value in Dataset.items():\n",
    "    print(f\"Key: '{key}'\")\n",
    "    print(f\"  - Type: {type(value)}\")\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  - Shape: {value.shape}\")\n",
    "    else:\n",
    "        print(\"  - No shape attribute.\")\n",
    "    if hasattr(value, 'dtype'):\n",
    "        print(f\"  - Dtype: {value.dtype}\")\n",
    "print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b799cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1932328\n",
      "2.932326\n",
      "4.1916647\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['displacements'][0][1])\n",
    "print(Dataset['target_e'][0])\n",
    "print(Dataset['target_e_prime'][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943c7c2",
   "metadata": {},
   "source": [
    "Node Classes and Acivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a77bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"Linear node for neural network\"\"\"\n",
    "\n",
    "    def __init__(self,din: int,dout: int,*,rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W = nnx.Param(jax.random.uniform(key=key, shape=(din,dout)))\n",
    "        self.b = nnx.Param(jnp.zeros(shape=(dout,)))\n",
    "        self.din, self.dout = din, dout\n",
    "\n",
    "    def __call__(self,x: jax.Array):\n",
    "        return(x @ self.W + self.b)\n",
    "    \n",
    "def SiLU(x: jax.Array):\n",
    "    \"\"\"Sigmoid Weighted Linear Unit activation function\"\"\"\n",
    "    return x * jax.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26f9bc",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bfbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class energy_prediction(nnx.Module):\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "\n",
    "    def __init__(self,dim_in: int, dim_hidden1_in: int, dim_hidden2_in: int,dim_hidden3_in, dim_out: int,*,rngs: nnx.Rngs):\n",
    "        self.layer1 = Linear(din=dim_in,dout=dim_hidden1_in,rngs=rngs)\n",
    "        self.layer2 = Linear(din=dim_hidden1_in,dout=dim_hidden2_in,rngs=rngs)\n",
    "        self.layer3 = Linear(din=dim_hidden2_in,dout=dim_hidden3_in,rngs=rngs)\n",
    "        self.layer4 = Linear(din=dim_hidden3_in,dout=dim_out,rngs=rngs)\n",
    "        self.silu = SiLU\n",
    "        \n",
    "    def __call__(self,x_in):\n",
    "        # pass to calculate e\n",
    "        def forwardPass(x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer4(x)\n",
    "            return x.squeeze()\n",
    "        \n",
    "        e = jax.vmap(forwardPass)(x_in)\n",
    "        dedx = jax.vmap(jax.grad(forwardPass,argnums=(0)))\n",
    "        e_prime = dedx(x_in)\n",
    "\n",
    "        return e, e_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc036a78",
   "metadata": {},
   "source": [
    "Define optimiser and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f469e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=Learn_Rate, b1=beta_1, b2=beta_2)\n",
    "\n",
    "def loss_fn(x: jax.Array, target_e, target_e_prime,*, Model,alpha,gamma,lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_e, prediction_e_prime = Model(x)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime)**2)\n",
    "\n",
    "    target_zero = 0\n",
    "    x_zero = jnp.zeros(x[0].shape)\n",
    "    x_zero = jnp.expand_dims(x_zero, axis=0)\n",
    "    prediction_zero, _ = Model(x_zero)\n",
    "    loss_zero = jnp.mean((prediction_zero - target_zero)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57659f0",
   "metadata": {},
   "source": [
    "Train State Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b764c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class TrainState(nnx.Object):\n",
    "    params: Any\n",
    "    graph_def: Any \n",
    "    state: Any\n",
    "    alpha: float \n",
    "    gamma: float \n",
    "    lambda_: float "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118860",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da6228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def training_step(params,state,opt_state,batch,*,graph_def,alpha,gamma,lambda_):\n",
    "\n",
    "    disp_in = batch['displacements']\n",
    "    e_target = batch['target_e']\n",
    "    e_prime_target = batch['target_e_prime']\n",
    "\n",
    "    def wrapped_loss_fn(params_,state_):\n",
    "        Model = nnx.merge(graph_def,params_,state_)\n",
    "        loss = loss_fn(\n",
    "            disp_in,\n",
    "            e_target,\n",
    "            e_prime_target,\n",
    "            Model=Model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss_fn, argnums=0)(params, state) \n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_state = state\n",
    "\n",
    "    return new_params, new_state, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88cb1",
   "metadata": {},
   "source": [
    "Batch Creator and test set creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9981b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_batch_dataset(dataset, batch_size, test_split=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets, then yields batches for each.\n",
    "    Returns: (train_batches, test_batches).\n",
    "    \"\"\"\n",
    "    N = dataset['displacements'].shape[0]\n",
    "    indices = jnp.arange(N)\n",
    "    if shuffle:\n",
    "        indices = jax.random.permutation(jax.random.PRNGKey(0), indices)\n",
    "    split_idx = int(N * (1 - test_split))\n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "\n",
    "    def batch_indices(idx):\n",
    "        batch_num = len(idx) // batch_size\n",
    "        for i in range(batch_num):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            batch = {key: value[batch_idx] for key, value in dataset.items()}\n",
    "            yield batch\n",
    "\n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e467e7e",
   "metadata": {},
   "source": [
    "Create test and train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df10eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, test_batches = split_and_batch_dataset(\n",
    "    Dataset, \n",
    "    Batch_size, \n",
    "    test_split=(1 - train_split), \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df560003",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23fdd255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     27\u001b[39m batch_count = \u001b[32m0\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_batches,desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     new_params, new_state, new_opt_state, loss_batch = training_step(\n\u001b[32m     32\u001b[39m         train_state.params,\n\u001b[32m     33\u001b[39m         train_state.state,\n\u001b[32m     34\u001b[39m         opt_state,\n\u001b[32m     35\u001b[39m         batch,\n\u001b[32m     36\u001b[39m         graph_def=train_state.graph_def,\n\u001b[32m     37\u001b[39m         alpha=train_state.alpha,\n\u001b[32m     38\u001b[39m         gamma=train_state.gamma,\n\u001b[32m     39\u001b[39m         lambda_=train_state.lambda_\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     42\u001b[39m     opt_state = new_opt_state\n\u001b[32m     43\u001b[39m     train_state.params = new_params\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env_two\\Lib\\site-packages\\flax\\nnx\\transforms\\compilation.py:431\u001b[39m, in \u001b[36mJitWrapped.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    430\u001b[39m   pure_args, pure_kwargs = \u001b[38;5;28mself\u001b[39m._get_pure_args_kwargs(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[38;5;28mself\u001b[39m.jitted_fn(\n\u001b[32m    432\u001b[39m     *pure_args, **pure_kwargs\n\u001b[32m    433\u001b[39m   )\n\u001b[32m    434\u001b[39m   out = \u001b[38;5;28mself\u001b[39m._get_non_pure_out(pure_args_out, pure_kwargs_out, pure_out)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env_two\\Lib\\site-packages\\flax\\nnx\\variablelib.py:1077\u001b[39m, in \u001b[36m_variable_state_unflatten\u001b[39m\u001b[34m(static, children)\u001b[39m\n\u001b[32m   1072\u001b[39m     node = x.raw_value\n\u001b[32m   1074\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m (node,), (x.type, metadata)\n\u001b[32m-> \u001b[39m\u001b[32m1077\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_variable_state_unflatten\u001b[39m(\n\u001b[32m   1078\u001b[39m   static: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[Variable[A]], \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, tp.Any], ...]],\n\u001b[32m   1079\u001b[39m   children: \u001b[38;5;28mtuple\u001b[39m[A],\n\u001b[32m   1080\u001b[39m ) -> VariableState[A]:\n\u001b[32m   1081\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m VariableState(\n\u001b[32m   1082\u001b[39m     \u001b[38;5;28mtype\u001b[39m=static[\u001b[32m0\u001b[39m],\n\u001b[32m   1083\u001b[39m     value=children[\u001b[32m0\u001b[39m],\n\u001b[32m   1084\u001b[39m     **\u001b[38;5;28mdict\u001b[39m(static[\u001b[32m1\u001b[39m]),\n\u001b[32m   1085\u001b[39m   )\n\u001b[32m   1088\u001b[39m jtu.register_pytree_with_keys(\n\u001b[32m   1089\u001b[39m   VariableState,\n\u001b[32m   1090\u001b[39m   partial(_variable_state_flatten, with_keys=\u001b[38;5;28;01mTrue\u001b[39;00m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1091\u001b[39m   _variable_state_unflatten,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1092\u001b[39m   flatten_func=partial(_variable_state_flatten, with_keys=\u001b[38;5;28;01mFalse\u001b[39;00m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1093\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = energy_prediction(\n",
    "    dim_in=input_dataset.shape[1], \n",
    "    dim_hidden1_in=2048,\n",
    "    dim_hidden2_in=1024,\n",
    "    dim_hidden3_in=512, \n",
    "    dim_out=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "graph_def,params,state = nnx.split(Model,nnx.Param,nnx.State)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "train_state = TrainState(\n",
    "    graph_def=graph_def,\n",
    "    params=params,\n",
    "    state=state,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    lambda_=lambda_\n",
    "    )\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(train_batches,desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "        \n",
    "        new_params, new_state, new_opt_state, loss_batch = training_step(\n",
    "            train_state.params,\n",
    "            train_state.state,\n",
    "            opt_state,\n",
    "            batch,\n",
    "            graph_def=train_state.graph_def,\n",
    "            alpha=train_state.alpha,\n",
    "            gamma=train_state.gamma,\n",
    "            lambda_=train_state.lambda_\n",
    "        )\n",
    "\n",
    "        opt_state = new_opt_state\n",
    "        train_state.params = new_params\n",
    "        train_state.state = new_state\n",
    "\n",
    "        running_loss += loss_batch\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = avg_loss = running_loss / batch_count if batch_count > 0 else 0.0\n",
    "    loss_record.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e850b4",
   "metadata": {},
   "source": [
    "Final model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0738a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class ModelData(nnx.Object):\n",
    "    graph_def: Any\n",
    "    params: Any\n",
    "    state: Any\n",
    "    trained: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62b354",
   "metadata": {},
   "source": [
    "Create Final model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78303138",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_def_trained = train_state.graph_def\n",
    "params_trained = train_state.params\n",
    "state_trained = train_state.state\n",
    "\n",
    "model_data = ModelData(\n",
    "    graph_def=graph_def_trained,\n",
    "    params=params_trained,\n",
    "    state = state_trained,\n",
    "    trained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc294f4",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a9d2d21310>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jnp.log10(jnp.array(loss_record)))\n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baeb33",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_abs_error(pred,target):\n",
    "    n1 = pred.shape[0]\n",
    "    n2 = target.shape[0]\n",
    "\n",
    "    if n1 != n2:\n",
    "        raise(\"Error: inputs must have matching shape\")\n",
    "    \n",
    "    return (jnp.sum(jnp.abs(pred - target)) / n1)\n",
    "\n",
    "def test_model(model_data, test_batches, Dataset_parameters,*,loss_fn, alpha, gamma, lambda_):\n",
    "\n",
    "    trained = model_data.trained\n",
    "    if not trained:\n",
    "        raise TypeError(\"Model is untrained, please train the model before evaluation\")\n",
    "\n",
    "    test_graph_def = model_data.graph_def\n",
    "    test_params = model_data.params\n",
    "    test_state = model_data.state\n",
    "\n",
    "    test_model = nnx.merge(test_graph_def,test_params,test_state)\n",
    "\n",
    "    loss_test = 0.0\n",
    "    test_count = 0\n",
    "\n",
    "    for batch in test_batches:\n",
    "        displacements_test = batch['displacements']\n",
    "        e_target_test = batch['target_e']\n",
    "        e_prime_target_test = batch['target_e_prime']\n",
    "\n",
    "        e_target_test = unscale_data(e_target_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_target_test = unscale_data(e_prime_target_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        e_pred_test, e_prime_pred_test = test_model(displacements_test)\n",
    "\n",
    "        e_pred_test = unscale_data(e_pred_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_pred_test = unscale_data(e_prime_pred_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        batch_loss_test = loss_fn(\n",
    "            displacements_test,\n",
    "            e_target_test,\n",
    "            e_prime_target_test,\n",
    "            Model=test_model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        loss_test += batch_loss_test\n",
    "        test_count += 1\n",
    "\n",
    "        avg_e_abs_error = avg_abs_error(e_pred_test,e_target_test)\n",
    "        avg_e_prime_abs_error = avg_abs_error(e_prime_pred_test,e_prime_target_test)\n",
    "\n",
    "    avg_loss_test = loss_test / test_count\n",
    "    zero_val_e, _ = test_model(jnp.zeros_like(test_batches[0]['displacements']))\n",
    "    test_e_zero_error = avg_abs_error(zero_val_e, jnp.zeros_like(zero_val_e))\n",
    "\n",
    "    return avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe3504",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average absolute error for e is 107.10755920410156 in the test set\n",
      "the average absolute error for e prime is 7772113.0 in the test set\n",
      "the absolute zero error for e is 0.00042629241943359375 in the test set\n",
      "The average loss across the training set is 248377584.0\n",
      "The average absolute error for e is 77.6896743774414 in the training set\n",
      "the average absolute error for e prime is 3001238.75 in the training set\n",
      "the absolute zero error for e is 0.00042629241943359375 in the training set\n"
     ]
    }
   ],
   "source": [
    "avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error = test_model(model_data,test_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    "avg_loss_training, avg_e_abs_error_training, avg_e_prime_abs_error_training, test_e_zero_error_training = test_model(model_data,train_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    " \n",
    "print(f\"The average absolute error for e is {avg_e_abs_error} in the test set\") \n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error} in the test set\") \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error} in the test set\") \n",
    "\n",
    "print(f\"The average loss across the training set is {avg_loss_test}\")\n",
    "print(f\"The average absolute error for e is {avg_e_abs_error_training} in the training set\")\n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error_training} in the training set\")  \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error_training} in the training set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_env_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
