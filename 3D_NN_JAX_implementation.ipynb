{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax.experimental import nnx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "002646d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 152, 3)\n",
      "(10000, 152, 3)\n",
      "(10000,)\n",
      "(10000, 152, 3)\n",
      "(20000, 152, 3)\n",
      "(20000, 1)\n",
      "(20000, 152, 3)\n"
     ]
    }
   ],
   "source": [
    "# Due to errors I was experiencing this seems to be the quickest fix I could find to allow me to unpickle the data\n",
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file_1 = r\"C:\\Users\\samue\\Downloads\\Simulation.pickle\"\n",
    "data_file_2 = r\"C:\\Users\\samue\\Downloads\\Simulation 2.pickle\"\n",
    "\n",
    "with open(data_file_1,\"rb\") as f:\n",
    "    data_unpickled_1 = pickle.load(f)\n",
    "\n",
    "with open(data_file_2,\"rb\") as f:\n",
    "    data_unpickled_2 = pickle.load(f)\n",
    "\n",
    "_,data_object_1 = data_unpickled_1\n",
    "_,data_object_2 = data_unpickled_2\n",
    "\n",
    "input_dataset_1 = jnp.array(data_object_1.Indata)\n",
    "#data_index_1 = data_object_1.i\n",
    "e_dataset_1 = jnp.array(data_object_1.SE)\n",
    "e_prime_dataset_1 = jnp.array(data_object_1.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)\n",
    "#data_index_2 = data_object_2.i\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)\n",
    "\n",
    "print(input_dataset_2.shape)\n",
    "print(input_dataset_1.shape)\n",
    "print(e_dataset_1.shape)\n",
    "print(e_prime_dataset_1.shape)\n",
    "\n",
    "input_dataset = jax.numpy.concatenate([input_dataset_1,input_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.concatenate([e_dataset_1, e_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.expand_dims(target_e_dataset,axis=1)\n",
    "target_e_prime_dataset = jax.numpy.concatenate([e_prime_dataset_1,e_prime_dataset_2],axis=0)\n",
    "print(input_dataset.shape)\n",
    "print(target_e_dataset.shape)\n",
    "print(target_e_prime_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5ebb",
   "metadata": {},
   "source": [
    "Redimensionalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redimensionalise(self):\n",
    "    self.Disp = jnp.zeros((self.Dims,self.Dims,self.Dims,3))\n",
    "    m = 0\n",
    "    for i in range(self.Dims):\n",
    "        for j in range(self.Dims):\n",
    "            for k in range(self.Dims):\n",
    "                if self.xInMesh[0][i,j,k] == 0 or self.xInMesh[0][i,j,k] == 1 or self.xInMesh[1][i,j,k] == 0 or self.xInMesh[1][i,j,k] == 1 or self.xInMesh[2][i,j,k] == 0 or self.xInMesh[2][i,j,k] == 1:\n",
    "                    self.Disp[i,j,k,:] = self.RandDisp[self.Index,m,:]\n",
    "                    m = m +1\n",
    "    return self.Disp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53d879",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "debbce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a6f5b",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9eb139a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 10\n",
    "alpha = 1.0\n",
    "gamma = 1.0\n",
    "lambda_ = 1.0\n",
    "Learn_Rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "Batch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328deec4",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4221141",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = input_dataset.shape[0] // Batch_size\n",
    "\n",
    "# input_dataset target_e_dataset target_e_prime_dataset \n",
    "\n",
    "input_dataset = input_dataset.reshape((20000,456))\n",
    "target_e_dataset = target_e_dataset.reshape((20000,))\n",
    "target_e_prime_dataset = target_e_prime_dataset.reshape((20000,456))\n",
    "\n",
    "Dataset = {\n",
    "    'displacements':input_dataset,\n",
    "    'target_e':target_e_dataset,\n",
    "    'target_e_prime':target_e_prime_dataset\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943c7c2",
   "metadata": {},
   "source": [
    "Node Classes and Acivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a77bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"Linear node for neural network\"\"\"\n",
    "\n",
    "    def __init__(self,din: int,dout: int,*,rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W = nnx.Param(jax.random.uniform(key=key, shape=(din,dout)))\n",
    "        self.b = nnx.Param(jnp.zeros(shape=(dout,)))\n",
    "        self.din, self.dout = din, dout\n",
    "\n",
    "    def __call__(self,x: jax.Array):\n",
    "        return(x @ self.W + self.b)\n",
    "    \n",
    "def SiLU(x: jax.Array):\n",
    "    \"\"\"Sigmoid Weighted Linear Unit activation function\"\"\"\n",
    "    return x * jax.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26f9bc",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59bfbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class energy_prediction(nnx.Module):\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "\n",
    "    def __init__(self,dim_in: int, dim_hidden1_in: int, dim_hidden2_in: int,dim_hidden3_in, dim_out: int,*,rngs: nnx.Rngs):\n",
    "        self.layer1 = Linear(din=dim_in,dout=dim_hidden1_in,rngs=rngs)\n",
    "        self.layer2 = Linear(din=dim_hidden1_in,dout=dim_hidden2_in,rngs=rngs)\n",
    "        self.layer3 = Linear(din=dim_hidden2_in,dout=dim_hidden3_in,rngs=rngs)\n",
    "        self.layer4 = Linear(din=dim_hidden3_in,dout=dim_out,rngs=rngs)\n",
    "        self.silu = SiLU\n",
    "        \n",
    "    def __call__(self,x_in):\n",
    "        # pass to calculate e\n",
    "        def forwardPass(x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer4(x)\n",
    "            return x.squeeze(-1)\n",
    "        \n",
    "        e = forwardPass(x_in)\n",
    "        dedx = jax.vmap(jax.grad(forwardPass,argnums=(0)))\n",
    "        e_prime = dedx(x_in)\n",
    "\n",
    "        return e, e_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc036a78",
   "metadata": {},
   "source": [
    "Define optimiser and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f469e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=Learn_Rate, b1=beta_1, b2=beta_2)\n",
    "\n",
    "def loss_fn(x: jax.Array, target_e, target_e_prime,*, model,alpha,gamma,lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "\n",
    "    prediction_e, prediction_e_prime = model(x)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime)**2)\n",
    "\n",
    "    target_zero = 0\n",
    "    x_zero = jnp.zeros(x[0].shape)\n",
    "    x_zero = jnp.expand_dims(x_zero, axis=0)\n",
    "    prediction_zero, _ = model(x_zero)\n",
    "    loss_zero = jnp.mean((prediction_zero - target_zero)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57659f0",
   "metadata": {},
   "source": [
    "Train State Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b764c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    def __init__(self,*,model_def,params,optimiser,opt_state,alpha,gamma,lambda_):\n",
    "        self.model_def = model_def\n",
    "        self.params = params\n",
    "        self.optimiser = optimiser\n",
    "        self.opt_state = opt_state\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def replace(self,*,model_def,params,optimiser,opt_state,alpha,gamma,lambda_):\n",
    "        self.model_def = model_def\n",
    "        self.params = params\n",
    "        self.optimiser = optimiser\n",
    "        self.opt_state = opt_state\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118860",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da6228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit(static_argnames=['params','optimiser','loss_fn'])\n",
    "def training_step(Model_def,params,optimiser,opt_state,alpha,gamma,lambda_,batch,loss_fn):\n",
    "    disp_in = batch['displacements']\n",
    "    e_target = batch['target_e']\n",
    "    e_prime_target = batch['target_e_prime']\n",
    "\n",
    "    def wrapped_loss_fn(params):\n",
    "        Model = nnx.merge(Model_def,params)\n",
    "        loss = loss_fn(\n",
    "            disp_in,\n",
    "            e_target,\n",
    "            e_prime_target,\n",
    "            model=Model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss = wrapped_loss_fn(params)\n",
    "    grads = nnx.grad(wrapped_loss_fn)(params)\n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    return new_params, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88cb1",
   "metadata": {},
   "source": [
    "Batch Creator and test set creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9981b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_batch_dataset(dataset, batch_size, test_split=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets, then yields batches for each.\n",
    "    Returns: (train_batches, test_batches)\n",
    "    \"\"\"\n",
    "    N = dataset['displacements'].shape[0]\n",
    "    indices = jnp.arange(N)\n",
    "    if shuffle:\n",
    "        indices = jax.random.permutation(jax.random.PRNGKey(0), indices)\n",
    "    split_idx = int(N * (1 - test_split))\n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "\n",
    "    def batch_indices(idx):\n",
    "        for start in range(0, len(idx), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            yield {key: value[batch_idx] for key, value in dataset.items()}\n",
    "\n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e467e7e",
   "metadata": {},
   "source": [
    "Create test and train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df10eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, test_batches = split_and_batch_dataset(\n",
    "    Dataset, \n",
    "    Batch_size, \n",
    "    test_split=0.2, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df560003",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23fdd255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "No attribute 'num_leaves' in State",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m batch_count = \u001b[32m0\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_batches,desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     new_params, new_opt_state, loss_batch = training_step(\n\u001b[32m     31\u001b[39m         Model_def=train_state.model_def,\n\u001b[32m     32\u001b[39m         opt_state=train_state.opt_state,\n\u001b[32m     33\u001b[39m         params=train_state.params,\n\u001b[32m     34\u001b[39m         optimiser=train_state.optimiser,\n\u001b[32m     35\u001b[39m         alpha=train_state.alpha,\n\u001b[32m     36\u001b[39m         gamma=train_state.gamma,\n\u001b[32m     37\u001b[39m         lambda_=train_state.lambda_,\n\u001b[32m     38\u001b[39m         batch=batch,\n\u001b[32m     39\u001b[39m         loss_fn=loss_fn\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     42\u001b[39m     train_state = train_state.replace(\n\u001b[32m     43\u001b[39m         model_def=train_state.model_def,\n\u001b[32m     44\u001b[39m         opt_state=new_opt_state,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m         lambda_=train_state.lambda_\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     52\u001b[39m     running_loss += loss_batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env\\Lib\\site-packages\\flax\\nnx\\transforms\\compilation.py:431\u001b[39m, in \u001b[36mJitWrapped.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m graph.update_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    430\u001b[39m   pure_args, pure_kwargs = \u001b[38;5;28mself\u001b[39m._get_pure_args_kwargs(args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = \u001b[38;5;28mself\u001b[39m.jitted_fn(\n\u001b[32m    432\u001b[39m     *pure_args, **pure_kwargs\n\u001b[32m    433\u001b[39m   )\n\u001b[32m    434\u001b[39m   out = \u001b[38;5;28mself\u001b[39m._get_non_pure_out(pure_args_out, pure_kwargs_out, pure_out)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "    \u001b[31m[... skipping hidden 16 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env\\Lib\\site-packages\\flax\\nnx\\transforms\\compilation.py:126\u001b[39m, in \u001b[36mJitFn.__call__\u001b[39m\u001b[34m(self, *pure_args, **pure_kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *pure_args, **pure_kwargs):\n\u001b[32m    119\u001b[39m   args, kwargs = extract.from_tree(\n\u001b[32m    120\u001b[39m     (pure_args, pure_kwargs),\n\u001b[32m    121\u001b[39m     merge_fn=_jit_merge_fn,\n\u001b[32m    122\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    123\u001b[39m     is_inner=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    124\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m   out = \u001b[38;5;28mself\u001b[39m.f(*args, **kwargs)\n\u001b[32m    128\u001b[39m   args_out, kwargs_out = extract.clear_non_graph_nodes((args, kwargs))\n\u001b[32m    129\u001b[39m   pure_args_out, pure_kwargs_out, pure_out = extract.to_tree(\n\u001b[32m    130\u001b[39m     (args_out, kwargs_out, out),\n\u001b[32m    131\u001b[39m     prefix=(\u001b[38;5;28mself\u001b[39m.in_shardings, \u001b[38;5;28mself\u001b[39m.kwarg_shardings, \u001b[38;5;28mself\u001b[39m.out_shardings),\n\u001b[32m    132\u001b[39m     ctxtag=\u001b[38;5;28mself\u001b[39m.ctxtag,\n\u001b[32m    133\u001b[39m     split_fn=_jit_split_fn,\n\u001b[32m    134\u001b[39m   )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mtraining_step\u001b[39m\u001b[34m(Model_def, params, optimiser, opt_state, alpha, gamma, lambda_, batch, loss_fn)\u001b[39m\n\u001b[32m      9\u001b[39m     loss = loss_fn(\n\u001b[32m     10\u001b[39m         disp_in,\n\u001b[32m     11\u001b[39m         e_target,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m         lam=lambda_\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m loss = wrapped_loss_fn(params)\n\u001b[32m     21\u001b[39m grads = nnx.grad(wrapped_loss_fn)(params)\n\u001b[32m     22\u001b[39m updates, new_opt_state = optimiser.update(grads, opt_state, params)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtraining_step.<locals>.wrapped_loss_fn\u001b[39m\u001b[34m(params)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_loss_fn\u001b[39m(params):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     Model = nnx.merge(Model_def,params)\n\u001b[32m      9\u001b[39m     loss = loss_fn(\n\u001b[32m     10\u001b[39m         disp_in,\n\u001b[32m     11\u001b[39m         e_target,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m         lam=lambda_\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env\\Lib\\site-packages\\flax\\nnx\\graph.py:2368\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(graphdef, state, *states)\u001b[39m\n\u001b[32m   2366\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2367\u001b[39m   _state = _merge_to_flat_state((state, *states))\n\u001b[32m-> \u001b[39m\u001b[32m2368\u001b[39m node = unflatten(graphdef, _state)\n\u001b[32m   2369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env\\Lib\\site-packages\\flax\\nnx\\graph.py:1106\u001b[39m, in \u001b[36munflatten\u001b[39m\u001b[34m(graphdef, state, index_ref, outer_index_outer_ref)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_ref \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1104\u001b[39m   index_ref = IndexMap()\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(leaves) != graphdef.num_leaves:\n\u001b[32m   1107\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1108\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIncorrect number of leaves, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraphdef.num_leaves\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m leaves, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(leaves)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1109\u001b[39m   )\n\u001b[32m   1111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nodedef := graphdef.nodes[\u001b[32m0\u001b[39m], NodeRef):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\samue\\miniconda3\\envs\\JAX_ML_env\\Lib\\site-packages\\flax\\nnx\\statelib.py:267\u001b[39m, in \u001b[36mState.__getattr__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: K) -> State | V:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    266\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m_mapping\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in State\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    268\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n",
      "\u001b[31mAttributeError\u001b[39m: No attribute 'num_leaves' in State"
     ]
    }
   ],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = energy_prediction(\n",
    "    dim_in=input_dataset.shape[1], \n",
    "    dim_hidden1_in=2024,\n",
    "    dim_hidden2_in=1012,\n",
    "    dim_hidden3_in=212, \n",
    "    dim_out=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "params, Model_def = nnx.split(Model,nnx.Param)\n",
    "opt_state = optimiser.init(params)\n",
    "train_state = TrainState(\n",
    "    model_def=Model_def,\n",
    "    params=params,\n",
    "    optimiser=optimiser,\n",
    "    opt_state=opt_state,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    lambda_=lambda_,\n",
    "    )\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(train_batches,desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "        new_params, new_opt_state, loss_batch = training_step(\n",
    "            Model_def=train_state.model_def,\n",
    "            opt_state=train_state.opt_state,\n",
    "            params=train_state.params,\n",
    "            optimiser=train_state.optimiser,\n",
    "            alpha=train_state.alpha,\n",
    "            gamma=train_state.gamma,\n",
    "            lambda_=train_state.lambda_,\n",
    "            batch=batch,\n",
    "            loss_fn=loss_fn\n",
    "        )\n",
    "\n",
    "        train_state = train_state.replace(\n",
    "            model_def=train_state.model_def,\n",
    "            opt_state=new_opt_state,\n",
    "            params=new_params,\n",
    "            optimiser=train_state.optimiser,\n",
    "            alpha=train_state.alpha,\n",
    "            gamma=train_state.gamma,\n",
    "            lambda_=train_state.lambda_\n",
    "        )\n",
    "\n",
    "        running_loss += loss_batch\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = avg_loss = running_loss / batch_count if batch_count > 0 else 0.0\n",
    "    loss_record.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e850b4",
   "metadata": {},
   "source": [
    "Final model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0738a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData:\n",
    "    def __init__(self,*,model_def,params,trained):\n",
    "        self.model_def = model_def\n",
    "        self.params = params\n",
    "        self.trained = trained\n",
    "\n",
    "    def is_trained(self):\n",
    "        if self.trained:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def evaluate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62b354",
   "metadata": {},
   "source": [
    "Create Final model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78303138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_def_trained = train_state.model_def\n",
    "params_trained = train_state.params\n",
    "\n",
    "model_data = ModelData(\n",
    "    model_def=model_def_trained,\n",
    "    params=params_trained,\n",
    "    trained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc294f4",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f93141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23ed9cb5090>]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4b0a9",
   "metadata": {},
   "source": [
    "Eval State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d94470",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'flax.nnx' has no attribute 'get_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[437]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mself\u001b[39m.gamma = gamma\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mself\u001b[39m.lambda_ = lambda_\n\u001b[32m     11\u001b[39m evaluation_state = Eval_state(\n\u001b[32m     12\u001b[39m     model=test_model,\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     params=nnx.get_parameters(test_model),\n\u001b[32m     14\u001b[39m     alpha=alpha,\n\u001b[32m     15\u001b[39m     gamma=gamma,\n\u001b[32m     16\u001b[39m     lambda_=lambda_\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: module 'flax.nnx' has no attribute 'get_parameters'"
     ]
    }
   ],
   "source": [
    "test_model = train_state.model\n",
    "\n",
    "class Eval_state:\n",
    "    def __init__(self,model,params,alpha,gamma,lambda_):\n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "evaluation_state = Eval_state(\n",
    "    model=test_model,\n",
    "    params=1,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    lambda_=lambda_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baeb33",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1645b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_abs_error(pred,target):\n",
    "    n1 = pred.shape[0]\n",
    "    n2 = target.shape[0]\n",
    "\n",
    "    if n1 != n2:\n",
    "        raise(\"Error: inputs must have matching shape\")\n",
    "    \n",
    "    return (jnp.sum(jnp.abs(pred - target)) / n1)\n",
    "\n",
    "def test_model(evaluation_state,Batch_size,*,loss_fn):\n",
    "    test_model = evaluation_state.model\n",
    "    alpha = evaluation_state.alpha\n",
    "    gamma = evaluation_state.gamma\n",
    "    lambda_ = evaluation_state.lambda_\n",
    "    \n",
    "    loss_test = 0.0\n",
    "\n",
    "    for batch in test_batches:\n",
    "        displacements_test = batch['displacements']\n",
    "        e_target_test = batch['target_e']\n",
    "        e_prime_target_test = batch['target_e_prime']\n",
    "\n",
    "        e_pred_test, e_prime_pred_test = test_model(displacements_test)\n",
    "\n",
    "        batch_loss_test = loss_fn(\n",
    "            displacements_test,\n",
    "            e_target_test,\n",
    "            e_prime_target_test,\n",
    "            test_model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        loss_test += batch_loss_test\n",
    "\n",
    "        avg_e_abs_error = avg_abs_error(e_pred_test,e_target_test)\n",
    "        avg_e_prime_abs_error = avg_abs_error(e_prime_pred_test,e_prime_target_test)\n",
    "\n",
    "    avg_loss_test = loss_test / Batch_size\n",
    "    zero_val_e,zero_val_e_prime = test_model(jnp.zeros())\n",
    "    test_e_zero_error = avg_abs_error(zero_val_e, jnp.zeros_like(zero_val_e))\n",
    "    test_e_prime_zero_error = avg_abs_error(zero_val_e_prime, jnp.zeros_like(zero_val_e_prime))\n",
    "\n",
    "    return avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error, test_e_prime_zero_error\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
