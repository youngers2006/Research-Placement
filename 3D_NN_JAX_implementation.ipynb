{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax import nnx\n",
    "from flax import struct\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc34597",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cae85666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 1000\n",
    "alpha = 1.0\n",
    "gamma = 0.4\n",
    "lambda_ = 0.1\n",
    "Learn_Rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "Batch_size = 40\n",
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002646d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2340, 152, 3)\n",
      "(10000, 152, 3)\n",
      "(10000,)\n",
      "(10000, 152, 3)\n",
      "(12340, 152, 3)\n",
      "(12340, 1)\n",
      "(12340, 152, 3)\n"
     ]
    }
   ],
   "source": [
    "# Due to errors I was experiencing this seems to be the quickest fix I could find to allow me to unpickle the data\n",
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file_1 = r\"C:\\Users\\samue\\Downloads\\Simulation.pickle\"\n",
    "data_file_2 = r\"C:\\Users\\samue\\Downloads\\Simulation 2.pickle\"\n",
    "\n",
    "with open(data_file_1,\"rb\") as f:\n",
    "    data_unpickled_1 = pickle.load(f)\n",
    "\n",
    "with open(data_file_2,\"rb\") as f:\n",
    "    data_unpickled_2 = pickle.load(f)\n",
    "\n",
    "_,data_object_1 = data_unpickled_1\n",
    "_,data_object_2 = data_unpickled_2\n",
    "\n",
    "input_dataset_1 = jnp.array(data_object_1.Indata)\n",
    "#data_index_1 = data_object_1.i\n",
    "e_dataset_1 = jnp.array(data_object_1.SE)\n",
    "e_prime_dataset_1 = jnp.array(data_object_1.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)\n",
    "#data_index_2 = data_object_2.i\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)\n",
    "\n",
    "input_dataset_2 = jnp.array(data_object_2.Indata)[0:2340]\n",
    "e_dataset_2 = jnp.array(data_object_2.SE)[0:2340]\n",
    "e_prime_dataset_2 = jnp.array(data_object_2.Jac)[0:2340]\n",
    "\n",
    "print(input_dataset_2.shape)\n",
    "print(input_dataset_1.shape)\n",
    "print(e_dataset_1.shape)\n",
    "print(e_prime_dataset_1.shape)\n",
    "\n",
    "input_dataset = jax.numpy.concatenate([input_dataset_1,input_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.concatenate([e_dataset_1, e_dataset_2],axis=0)\n",
    "target_e_dataset = jax.numpy.expand_dims(target_e_dataset,axis=1)\n",
    "target_e_prime_dataset = jax.numpy.concatenate([e_prime_dataset_1,e_prime_dataset_2],axis=0)\n",
    "\n",
    "print(input_dataset.shape)\n",
    "print(target_e_dataset.shape)\n",
    "print(target_e_prime_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5ebb",
   "metadata": {},
   "source": [
    "Redimensionalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb876813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Redimensionalise(self):\n",
    "    self.Disp = jnp.zeros((self.Dims,self.Dims,self.Dims,3))\n",
    "    m = 0\n",
    "    for i in range(self.Dims):\n",
    "        for j in range(self.Dims):\n",
    "            for k in range(self.Dims):\n",
    "                if self.xInMesh[0][i,j,k] == 0 or self.xInMesh[0][i,j,k] == 1 or self.xInMesh[1][i,j,k] == 0 or self.xInMesh[1][i,j,k] == 1 or self.xInMesh[2][i,j,k] == 0 or self.xInMesh[2][i,j,k] == 1:\n",
    "                    self.Disp[i,j,k,:] = self.RandDisp[self.Index,m,:]\n",
    "                    m = m +1\n",
    "    return self.Disp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53d879",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debbce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec14bee",
   "metadata": {},
   "source": [
    "Pre and post processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6607d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data = data[:split_idx]\n",
    "    \n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "\n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328deec4",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4221141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSPECTING RAW DATASET\n",
      "Key: 'displacements'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340,)\n",
      "  - Dtype: float32\n",
      "Key: 'target_e_prime'\n",
      "  - Type: <class 'jaxlib._jax.ArrayImpl'>\n",
      "  - Shape: (12340, 456)\n",
      "  - Dtype: float32\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_num = input_dataset.shape[0] // Batch_size\n",
    "\n",
    "input_dataset = input_dataset.reshape((input_dataset.shape[0],456))\n",
    "target_e_dataset = target_e_dataset.reshape((target_e_dataset.shape[0],))\n",
    "target_e_prime_dataset = target_e_prime_dataset.reshape((target_e_prime_dataset.shape[0],456))\n",
    "\n",
    "params_dict_displacement = mean_and_std_dev(input_dataset,train_split=train_split)\n",
    "params_dict_target_e = mean_and_std_dev(target_e_dataset,train_split=train_split)\n",
    "params_dict_target_e_prime = mean_and_std_dev(target_e_prime_dataset,train_split=train_split)\n",
    "\n",
    "input_dataset_scaled = scale_data(input_dataset, data_params=params_dict_displacement)\n",
    "target_e_dataset_scaled = scale_data(target_e_dataset, data_params=params_dict_target_e)\n",
    "target_e_prime_dataset_scaled = scale_data(target_e_prime_dataset, data_params=params_dict_target_e_prime)\n",
    "\n",
    "Dataset_parameters = {\n",
    "    'displacements':params_dict_displacement,\n",
    "    'target_e':params_dict_target_e,\n",
    "    'target_e_prime':params_dict_target_e_prime\n",
    "}\n",
    "\n",
    "Dataset = {\n",
    "    'displacements':input_dataset_scaled,\n",
    "    'target_e':target_e_dataset_scaled,\n",
    "    'target_e_prime':target_e_prime_dataset_scaled\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"INSPECTING RAW DATASET\")\n",
    "for key, value in Dataset.items():\n",
    "    print(f\"Key: '{key}'\")\n",
    "    print(f\"  - Type: {type(value)}\")\n",
    "    if hasattr(value, 'shape'):\n",
    "        print(f\"  - Shape: {value.shape}\")\n",
    "    else:\n",
    "        print(\"  - No shape attribute.\")\n",
    "    if hasattr(value, 'dtype'):\n",
    "        print(f\"  - Dtype: {value.dtype}\")\n",
    "print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b799cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1932328\n",
      "2.932326\n",
      "4.1916647\n"
     ]
    }
   ],
   "source": [
    "print(Dataset['displacements'][0][1])\n",
    "print(Dataset['target_e'][0])\n",
    "print(Dataset['target_e_prime'][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943c7c2",
   "metadata": {},
   "source": [
    "Node Classes and Acivations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a77bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"Linear node for neural network\"\"\"\n",
    "\n",
    "    def __init__(self,din: int,dout: int,*,rngs: nnx.Rngs):\n",
    "        key = rngs.params()\n",
    "        self.W = nnx.Param(jax.random.uniform(key=key, shape=(din,dout)))\n",
    "        self.b = nnx.Param(jnp.zeros(shape=(dout,)))\n",
    "        self.din, self.dout = din, dout\n",
    "\n",
    "    def __call__(self,x: jax.Array):\n",
    "        return(x @ self.W + self.b)\n",
    "    \n",
    "def SiLU(x: jax.Array):\n",
    "    \"\"\"Sigmoid Weighted Linear Unit activation function\"\"\"\n",
    "    return x * jax.nn.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26f9bc",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bfbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class energy_prediction(nnx.Module):\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "\n",
    "    def __init__(self,dim_in: int, dim_hidden1_in: int, dim_hidden2_in: int,dim_hidden3_in, dim_out: int,*,rngs: nnx.Rngs):\n",
    "        self.layer1 = Linear(din=dim_in,dout=dim_hidden1_in,rngs=rngs)\n",
    "        self.layer2 = Linear(din=dim_hidden1_in,dout=dim_hidden2_in,rngs=rngs)\n",
    "        self.layer3 = Linear(din=dim_hidden2_in,dout=dim_hidden3_in,rngs=rngs)\n",
    "        self.layer4 = Linear(din=dim_hidden3_in,dout=dim_out,rngs=rngs)\n",
    "        self.silu = SiLU\n",
    "        \n",
    "    def __call__(self,x_in):\n",
    "        # pass to calculate e\n",
    "        def forwardPass(x):\n",
    "            x = self.layer1(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer3(x)\n",
    "            x = self.silu(x)\n",
    "            x = self.layer4(x)\n",
    "            return x.squeeze()\n",
    "        \n",
    "        e = jax.vmap(forwardPass)(x_in)\n",
    "        dedx = jax.vmap(jax.grad(forwardPass,argnums=(0)))\n",
    "        e_prime = dedx(x_in)\n",
    "\n",
    "        return e, e_prime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc036a78",
   "metadata": {},
   "source": [
    "Define optimiser and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f469e747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.adam(learning_rate=Learn_Rate, b1=beta_1, b2=beta_2)\n",
    "\n",
    "def loss_fn(x: jax.Array, target_e, target_e_prime,*, Model,alpha,gamma,lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_e, prediction_e_prime = Model(x)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime)**2)\n",
    "\n",
    "    target_zero = 0\n",
    "    x_zero = jnp.zeros(x[0].shape)\n",
    "    x_zero = jnp.expand_dims(x_zero, axis=0)\n",
    "    prediction_zero, _ = Model(x_zero)\n",
    "    loss_zero = jnp.mean((prediction_zero - target_zero)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57659f0",
   "metadata": {},
   "source": [
    "Train State Bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b764c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class TrainState(nnx.Object):\n",
    "    params: Any\n",
    "    graph_def: Any \n",
    "    state: Any\n",
    "    alpha: float \n",
    "    gamma: float \n",
    "    lambda_: float "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84118860",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da6228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def training_step(params,state,opt_state,batch,*,graph_def,alpha,gamma,lambda_):\n",
    "\n",
    "    disp_in = batch['displacements']\n",
    "    e_target = batch['target_e']\n",
    "    e_prime_target = batch['target_e_prime']\n",
    "\n",
    "    def wrapped_loss_fn(params_,state_):\n",
    "        Model = nnx.merge(graph_def,params_,state_)\n",
    "        loss = loss_fn(\n",
    "            disp_in,\n",
    "            e_target,\n",
    "            e_prime_target,\n",
    "            Model=Model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss_fn, argnums=0)(params, state) \n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_state = state\n",
    "\n",
    "    return new_params, new_state, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e88cb1",
   "metadata": {},
   "source": [
    "Batch Creator and test set creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9981b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_batch_dataset(dataset, batch_size, test_split=0.2, shuffle=True):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and test sets, then yields batches for each.\n",
    "    Returns: (train_batches, test_batches).\n",
    "    \"\"\"\n",
    "    N = dataset['displacements'].shape[0]\n",
    "    indices = jnp.arange(N)\n",
    "    if shuffle:\n",
    "        indices = jax.random.permutation(jax.random.PRNGKey(0), indices)\n",
    "    split_idx = int(N * (1 - test_split))\n",
    "    train_idx = indices[:split_idx]\n",
    "    test_idx = indices[split_idx:]\n",
    "\n",
    "    def batch_indices(idx):\n",
    "        batch_num = len(idx) // batch_size\n",
    "        for i in range(batch_num):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            batch = {key: value[batch_idx] for key, value in dataset.items()}\n",
    "            yield batch\n",
    "\n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    return train_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e467e7e",
   "metadata": {},
   "source": [
    "Create test and train batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df10eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, test_batches = split_and_batch_dataset(\n",
    "    Dataset, \n",
    "    Batch_size, \n",
    "    test_split=(1 - train_split), \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df560003",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fdd255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = energy_prediction(\n",
    "    dim_in=input_dataset.shape[1], \n",
    "    dim_hidden1_in=1024,\n",
    "    dim_hidden2_in=512,\n",
    "    dim_hidden3_in=256, \n",
    "    dim_hidden4_in=128,\n",
    "    dim_out=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "graph_def,params,state = nnx.split(Model,nnx.Param,nnx.State)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "train_state = TrainState(\n",
    "    graph_def=graph_def,\n",
    "    params=params,\n",
    "    state=state,\n",
    "    alpha=alpha,\n",
    "    gamma=gamma,\n",
    "    lambda_=lambda_\n",
    "    )\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "\n",
    "    for batch in tqdm(train_batches,desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "        \n",
    "        new_params, new_state, new_opt_state, loss_batch = training_step(\n",
    "            train_state.params,\n",
    "            train_state.state,\n",
    "            opt_state,\n",
    "            batch,\n",
    "            graph_def=train_state.graph_def,\n",
    "            alpha=train_state.alpha,\n",
    "            gamma=train_state.gamma,\n",
    "            lambda_=train_state.lambda_\n",
    "        )\n",
    "\n",
    "        opt_state = new_opt_state\n",
    "        train_state.params = new_params\n",
    "        train_state.state = new_state\n",
    "\n",
    "        running_loss += loss_batch\n",
    "        batch_count += 1\n",
    "    \n",
    "    avg_loss = avg_loss = running_loss / batch_count if batch_count > 0 else 0.0\n",
    "    loss_record.append(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e850b4",
   "metadata": {},
   "source": [
    "Final model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0738a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class ModelData(nnx.Object):\n",
    "    graph_def: Any\n",
    "    params: Any\n",
    "    state: Any\n",
    "    trained: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62b354",
   "metadata": {},
   "source": [
    "Create Final model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78303138",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_def_trained = train_state.graph_def\n",
    "params_trained = train_state.params\n",
    "state_trained = train_state.state\n",
    "\n",
    "model_data = ModelData(\n",
    "    graph_def=graph_def_trained,\n",
    "    params=params_trained,\n",
    "    state = state_trained,\n",
    "    trained=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc294f4",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f93141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1dbea07c410>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(jnp.log10(jnp.array(loss_record)))\n",
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baeb33",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1645b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_abs_error(pred,target):\n",
    "    n1 = pred.shape[0]\n",
    "    n2 = target.shape[0]\n",
    "\n",
    "    if n1 != n2:\n",
    "        raise(\"Error: inputs must have matching shape\")\n",
    "    \n",
    "    return (jnp.sum(jnp.abs(pred - target)) / n1)\n",
    "\n",
    "def test_model(model_data, test_batches, Dataset_parameters,*,loss_fn, alpha, gamma, lambda_):\n",
    "\n",
    "    trained = model_data.trained\n",
    "    if not trained:\n",
    "        raise TypeError(\"Model is untrained, please train the model before evaluation\")\n",
    "\n",
    "    test_graph_def = model_data.graph_def\n",
    "    test_params = model_data.params\n",
    "    test_state = model_data.state\n",
    "\n",
    "    test_model = nnx.merge(test_graph_def,test_params,test_state)\n",
    "\n",
    "    loss_test = 0.0\n",
    "    test_count = 0\n",
    "\n",
    "    for batch in test_batches:\n",
    "        displacements_test = batch['displacements']\n",
    "        e_target_test = batch['target_e']\n",
    "        e_prime_target_test = batch['target_e_prime']\n",
    "\n",
    "        e_target_test = unscale_data(e_target_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_target_test = unscale_data(e_prime_target_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        e_pred_test, e_prime_pred_test = test_model(displacements_test)\n",
    "\n",
    "        e_pred_test = unscale_data(e_pred_test,data_params=Dataset_parameters['target_e'])\n",
    "        e_prime_pred_test = unscale_data(e_prime_pred_test,data_params=Dataset_parameters['target_e_prime'])\n",
    "\n",
    "        batch_loss_test = loss_fn(\n",
    "            displacements_test,\n",
    "            e_target_test,\n",
    "            e_prime_target_test,\n",
    "            Model=test_model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        loss_test += batch_loss_test\n",
    "        test_count += 1\n",
    "\n",
    "        avg_e_abs_error = avg_abs_error(e_pred_test,e_target_test)\n",
    "        avg_e_prime_abs_error = avg_abs_error(e_prime_pred_test,e_prime_target_test)\n",
    "\n",
    "    avg_loss_test = loss_test / test_count\n",
    "    zero_val_e, _ = test_model(jnp.zeros_like(test_batches[0]['displacements']))\n",
    "    test_e_zero_error = avg_abs_error(zero_val_e, jnp.zeros_like(zero_val_e))\n",
    "\n",
    "    return avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe3504",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1486d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average absolute error for e is 111.77091217041016 in the test set\n",
      "the average absolute error for e prime is 7540245.5 in the test set\n",
      "the absolute zero error for e is 0.036048054695129395 in the test set\n",
      "The average loss across the training set is 252671280.0\n",
      "The average absolute error for e is 79.17456817626953 in the training set\n",
      "the average absolute error for e prime is 3000347.25 in the training set\n",
      "the absolute zero error for e is 0.036048054695129395 in the training set\n"
     ]
    }
   ],
   "source": [
    "avg_loss_test, avg_e_abs_error, avg_e_prime_abs_error, test_e_zero_error = test_model(model_data,test_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    "avg_loss_training, avg_e_abs_error_training, avg_e_prime_abs_error_training, test_e_zero_error_training = test_model(model_data,train_batches, Dataset_parameters,loss_fn=loss_fn,alpha=alpha,gamma=gamma,lambda_=lambda_)\n",
    " \n",
    "print(f\"The average absolute error for e is {avg_e_abs_error} in the test set\") \n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error} in the test set\") \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error} in the test set\") \n",
    "\n",
    "print(f\"The average loss across the training set is {avg_loss_test}\")\n",
    "print(f\"The average absolute error for e is {avg_e_abs_error_training} in the training set\")\n",
    "print(f\"the average absolute error for e prime is {avg_e_prime_abs_error_training} in the training set\")  \n",
    "print(f\"the absolute zero error for e is {test_e_zero_error_training} in the training set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_env_two",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
