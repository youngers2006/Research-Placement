{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "from flax import nnx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import jraph\n",
    "from itertools import combinations\n",
    "import meshio\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78afd1",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2b53ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 500\n",
    "alpha = 1.0\n",
    "gamma = 1.0\n",
    "lambda_ = 1.0\n",
    "beta_1 = 0.999\n",
    "beta_2 = 0.9\n",
    "batch_size = 40\n",
    "train_split = 0.9\n",
    "CV_split = 0.05\n",
    "test_split = 0.05\n",
    "Learn_Rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd846b",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f457fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992108e",
   "metadata": {},
   "source": [
    "Graph gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_known(boundary_points, points):\n",
    "    is_known = jnp.zeros(points.shape[0]) \n",
    "    is_known = is_known.at[boundary_points].set(1)\n",
    "    return is_known\n",
    "\n",
    "def build_send_receive(cell):\n",
    "    sender_array = []\n",
    "    receiver_array = []\n",
    "    for edge in combinations(cell,2):\n",
    "        sender_array.append(edge[0])\n",
    "        receiver_array.append(edge[1])\n",
    "    return sender_array, receiver_array\n",
    "\n",
    "def build_graphs(senders, receivers, positions, boundary_points, U) -> jraph.GraphsTuple:\n",
    "    is_known = Get_known(boundary_points, positions)\n",
    "    U_applied = jnp.zeros_like(U).at[boundary_points].set(U[boundary_points])\n",
    "        \n",
    "    node_features = jnp.concatenate([positions, U_applied, is_known], axis=1)\n",
    "    num_nodes = positions.shape[0]\n",
    "\n",
    "    graph = jraph.GraphsTuple(\n",
    "        nodes=node_features,\n",
    "        senders=senders,\n",
    "        receivers=receivers,\n",
    "        edges=None,\n",
    "        globals=None, \n",
    "        n_node=jnp.array([num_nodes]),\n",
    "        n_edge=jnp.array([len(senders)])\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab417de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete.\n",
      "\n",
      "Positions array shape: (1331, 3)\n",
      "Boundary indices array shape: (602,)\n",
      "Senders array shape: (14230,)\n",
      "Receivers array shape: (14230,)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your result file\n",
    "filepath = os.path.join('data', 'vtk', 'u_final.vtu')\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Error: '{filepath}' not found. Please check the file path.\")\n",
    "else:\n",
    "    mesh = meshio.read(filepath)\n",
    "\n",
    "    positions = mesh.points\n",
    "    right_face_indices = np.where(np.isclose(positions[:, 0], 1.0))[0]\n",
    "    element_connectivity = mesh.cells[0].data\n",
    "\n",
    "    unique_edges = set()\n",
    "\n",
    "    for element in element_connectivity:\n",
    "        element_senders, element_receivers = build_send_receive(element)\n",
    "        \n",
    "        for i in range(len(element_senders)):\n",
    "            edge = tuple(sorted((element_senders[i], element_receivers[i])))\n",
    "            unique_edges.add(edge)\n",
    "\n",
    "    edge_list = jnp.array(list(unique_edges))\n",
    "    senders = edge_list[:, 0]\n",
    "    receivers = edge_list[:, 1]\n",
    "\n",
    "    on_face_x0 = np.isclose(positions[:, 0], 0.0)\n",
    "    on_face_x1 = np.isclose(positions[:, 0], 1.0)\n",
    "    on_face_y0 = np.isclose(positions[:, 1], 0.0)\n",
    "    on_face_y1 = np.isclose(positions[:, 1], 1.0)\n",
    "    on_face_z0 = np.isclose(positions[:, 2], 0.0)\n",
    "    on_face_z1 = np.isclose(positions[:, 2], 1.0)\n",
    "\n",
    "    is_on_any_face = (on_face_x0 | on_face_x1 |\n",
    "                      on_face_y0 | on_face_y1 |\n",
    "                      on_face_z0 | on_face_z1)\n",
    "\n",
    "    boundary_nodes = np.where(is_on_any_face)[0]\n",
    "\n",
    "    print(\"Data extraction complete.\\n\")\n",
    "    print(f\"Positions array shape: {positions.shape}\")\n",
    "    print(f\"Boundary indices array shape: {boundary_nodes.shape}\")\n",
    "    print(f\"Senders array shape: {senders.shape}\")\n",
    "    print(f\"Receivers array shape: {receivers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202054",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8994d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1331, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file = r\"/home/samuel/Github/Research-Placement/data/simulation_results.pkl\"\n",
    "\n",
    "with open(data_file,\"rb\") as f:\n",
    "    data_unpickled_1 = pickle.load(f)\n",
    "\n",
    "dataset_dict = data_unpickled_1\n",
    "\n",
    "# Not tunable, is known from how many sims ran\n",
    "num_sims = 5\n",
    "# permutation list for batching\n",
    "index_list = jnp.arange(num_sims)\n",
    "permutated_index_list = jax.random.permutation(jax.random.PRNGKey(0), index_list)\n",
    "\n",
    "print(dataset_dict[1]['boundary_strain_energy_gradient'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183f4bc",
   "metadata": {},
   "source": [
    "Pre-processing functions - Need to be changed when preprocessing is implemented to accomodate the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*,train_split):\n",
    "    split_idx = int(data.shape[0] * train_split)\n",
    "    train_data = data[:split_idx]\n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a8d00",
   "metadata": {},
   "source": [
    "Data pre-processing and graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "# Data preprocessing is ignored for now\n",
    "processed_dataset_dict = dataset_dict\n",
    "\n",
    "graphs_list = []\n",
    "displacements_list = []\n",
    "target_e_list = []\n",
    "target_e_prime_list = []\n",
    "\n",
    "# Graph Building\n",
    "for i in range(num_sims):\n",
    "    U = processed_dataset_dict[i]['full_displacement_vector']\n",
    "    U = jnp.array(U)\n",
    "    graph = build_graphs(senders, receivers, positions, boundary_nodes, U)\n",
    "    graphs_list.append(graph)\n",
    "    displacements_list.append(U)\n",
    "    target_e = jnp.array(processed_dataset_dict[i]['strain_energy'])\n",
    "    target_e_list.append(target_e)\n",
    "    target_e_prime = jnp.array(processed_dataset_dict[i]['boundary_strain_energy_gradient'])\n",
    "    target_e_prime_list.append(target_e_prime)\n",
    "\n",
    "dataset = {\n",
    "    'graphs_list': graphs_list,\n",
    "    'displacements': displacements_list,\n",
    "    'target_e': target_e_list,\n",
    "    'target_e_prime': target_e_prime_list\n",
    "}\n",
    "\n",
    "param_dict = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eda85",
   "metadata": {},
   "source": [
    "Batching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_and_split_dataset(dataset_dict, batch_size, train_split, CV_split, test_split, permutated_index_list, shuffle=True):\n",
    "\n",
    "    n_train_samples = int(train_split * permutated_index_list.shape[0]) \n",
    "    n_test_samples = int(test_split * permutated_index_list.shape[0]) \n",
    "    n_CV_samples = int(CV_split * permutated_index_list.shape[0]) \n",
    "\n",
    "    if shuffle:\n",
    "        train_idx = permutated_index_list[:n_train_samples]\n",
    "        test_idx = permutated_index_list[n_train_samples:(n_train_samples + n_test_samples)]\n",
    "        CV_idx = permutated_index_list[(n_train_samples + n_test_samples):]\n",
    "    else:\n",
    "        index_list = range(len(permutated_index_list))\n",
    "        train_idx = index_list[:n_train_samples]\n",
    "        test_idx = index_list[n_train_samples:(n_train_samples + n_test_samples)]\n",
    "        CV_idx = index_list[(n_train_samples + n_test_samples):]\n",
    "\n",
    "    def batch_indices(idx):\n",
    "        num_batches = len(idx) // batch_size\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "\n",
    "            graphs_in_batch = [dataset_dict['graphs_list'][i] for i in batch_idx]\n",
    "            displacements_batch = dataset_dict['displacements'][batch_idx]\n",
    "            e_batch = dataset_dict['target_e'][batch_idx]\n",
    "            e_prime_batch = dataset_dict['target_e_prime'][batch_idx]\n",
    "\n",
    "            batched_graphs = jraph.batch(graphs_in_batch)\n",
    "            batched_displacements = jnp.array(displacements_batch)\n",
    "            batched_e = jnp.array(e_batch)\n",
    "            batched_e_prime = jnp.array(e_prime_batch)\n",
    "\n",
    "            yield {'graphs': batched_graphs, 'displacements': batched_displacements, 'target_e': batched_e, 'target_e_prime': batched_e_prime}\n",
    "    \n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    CV_batches = list(batch_indices(CV_idx))\n",
    "\n",
    "    return train_batches, CV_batches, test_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9a939",
   "metadata": {},
   "source": [
    "Batching graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b07a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches, CV_batches, test_batches = batch_and_split_dataset(\n",
    "    dataset_dict, \n",
    "    batch_size, \n",
    "    train_split, \n",
    "    CV_split, \n",
    "    test_split, \n",
    "    permutated_index_list, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c22883",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Silu(x: nnx.Array) -> nnx.Array:\n",
    "    return x * nnx.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f255c09c",
   "metadata": {},
   "source": [
    "Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399db1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nnx.Module):\n",
    "    \"\"\"\n",
    "    Applies trainable linear transformation to input vector x\n",
    "    Inputs: x: din dimensional row vectors as matrix\n",
    "    Return: Transformed dout dimensional vector\n",
    "    Trainable Params: w: d dimensional row vector, b: d dimensional row vector\n",
    "    \"\"\"\n",
    "    def __init__(self, din: int, dout: int,*, rngs: nnx.Rngs):\n",
    "        self.din, self.dout = din, dout\n",
    "        key = rngs.params()\n",
    "        initialiser = nnx.initializers.lecun_normal()\n",
    "        self.w = nnx.Param(initialiser(key=key, shape=(din,dout)))\n",
    "        self.b = nnx.Param(initialiser(key=key, shape=(dout,)))\n",
    "    \n",
    "    def __call__(self, x: jnp.Array):\n",
    "        return x @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615042f",
   "metadata": {},
   "source": [
    "GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nnx.Module):\n",
    "    def __init__(self, in_features, out_features,*,rngs):\n",
    "        key = rngs.params()\n",
    "        initialiser = nnx.initializers.lecun_normal()\n",
    "        self.W = nnx.Param(initialiser(key=key, shape=(in_features, out_features)))\n",
    "        self.A = nnx.Param(initialiser(key=key, shape=(2 * out_features, 1)))\n",
    "        self.SoftMax = jraph.segment_softmax()\n",
    "        self.Leaky_Relu = nnx.leaky_relu()\n",
    "\n",
    "    def __call__(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "\n",
    "        if graph.n_node is None:\n",
    "            raise ValueError(\"GAT requires nodes to have features\")\n",
    "        \n",
    "        h_sender = graph.nodes[graph.senders] @ self.W\n",
    "        h_receiver = graph.nodes[graph.receivers] @ self.W\n",
    "\n",
    "        send_receive_features = jnp.concatenate([h_sender, h_receiver], axis=-1)\n",
    "        attention_scores = self.Leaky_Relu(send_receive_features @ self.A)\n",
    "        \n",
    "        attention_coefficients = self.SoftMax(\n",
    "            logits=attention_scores, \n",
    "            segments_ids=graph.receivers,\n",
    "            num_segments=graph.n_node\n",
    "        )\n",
    "\n",
    "        weighted_features = attention_coefficients * h_sender\n",
    "\n",
    "        aggregate_nodes = jraph.aggregate_edges_for_nodes(\n",
    "            graph=graph,\n",
    "            edge_features=weighted_features,\n",
    "            aggregate_fn=jnp.sum\n",
    "        )\n",
    "\n",
    "        return graph._replace(nodes=aggregate_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045c04d",
   "metadata": {},
   "source": [
    "SAGPool WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a377c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGPool(nnx.Module): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314af953",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nnx.Module):\n",
    "    def __init__(self, input_dim: int, embedding_dim: int, output_dim: int, rngs: nnx.Rngs):\n",
    "        self.embedding_layer = Linear(input_dim, embedding_dim, rngs=rngs)\n",
    "        self.decoding_layer = Linear(embedding_dim, output_dim, rngs=rngs)\n",
    "\n",
    "        self.ReLU = nnx.relu()\n",
    "\n",
    "        self.encoderL1 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL1 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL2 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL2 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL3 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "    \n",
    "    def embedder(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        embeddings = self.embedding_layer(nodes)\n",
    "        return graph._replace(nodes=embeddings)\n",
    "    \n",
    "    def apply_activation_and_res(self, graph: jraph.GraphsTuple, residual: nnx.Array) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        activated_nodes = self.ReLU(nodes) + residual\n",
    "        return graph._replace(nodes=activated_nodes)\n",
    "    \n",
    "    def apply_res(self, graph: jraph.GraphsTuple, residual: nnx.Array):\n",
    "        new_nodes = graph.nodes + residual\n",
    "        return graph._replace(nodes=new_nodes)\n",
    "        \n",
    "    def decoder(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple: # Switch to SAGPool when its finished\n",
    "        aggregate_nodes = jraph.aggregate_nodes(graph, jnp.sum)\n",
    "        return self.decoding_layer(aggregate_nodes)\n",
    "        \n",
    "    def forward_pass(self, G: jraph.GraphsTuple, use_running_average: bool) -> nnx.Array:\n",
    "        G = self.embedder(G)\n",
    "        res1 = G.nodes\n",
    "\n",
    "        G = self.encoderL1(G)\n",
    "        self.BatchNormL1.use_running_average = use_running_average\n",
    "        nodes_norm = self.BatchNormL1(G.nodes)\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res1)\n",
    "        res2 = G.nodes\n",
    "\n",
    "        G = self.encoderL2(G)\n",
    "        self.BatchNormL2.use_running_average = use_running_average\n",
    "        nodes_norm = self.BatchNormL2(G.nodes)\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res2)\n",
    "        res3 = G.nodes\n",
    "\n",
    "        G = self.encoderL3(G)\n",
    "        G = self.apply_res(G, res3)\n",
    "\n",
    "        e = self.decoder(G)\n",
    "        return e\n",
    "    \n",
    "    def __call__(self, G: jraph.GraphsTuple, use_running_average):\n",
    "\n",
    "        e = self.forward_pass(G, use_running_average)\n",
    "        grad_graph = jax.grad(self.forward_pass, argnums=0)(G, use_running_average)\n",
    "        e_prime = grad_graph.nodes[:,4:7]\n",
    "\n",
    "        return e, e_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9615288",
   "metadata": {},
   "source": [
    "Loss function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = optax.chain(\n",
    "    optax.add_decayed_weights(weight_decay=1e-5),\n",
    "    optax.adam(\n",
    "    learning_rate=Learn_Rate, \n",
    "    b1=beta_1, \n",
    "    b2=beta_2\n",
    "    )\n",
    ")\n",
    "\n",
    "def loss_fn(graph_batch, target_e, target_e_prime,*, Model, Dataset_parameters, alpha, gamma, lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_e, prediction_e_prime = Model(graph_batch, Dataset_parameters)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean(optax.huber_loss(prediction_e_prime, target_e_prime))\n",
    "\n",
    "    mean_e = Dataset_parameters['target_e']['mean']\n",
    "    std_dev_e = Dataset_parameters['target_e']['std_dev']\n",
    "    target_zero = (0 - mean_e) / std_dev_e\n",
    "    \n",
    "    zero_graph = jraph.graphstuple(\n",
    "        \n",
    "    )\n",
    "    prediction_zero, _ = Model(zero_graph, Dataset_parameters)\n",
    "    loss_zero = jnp.mean((prediction_zero - target_zero)**2)\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd37a6b",
   "metadata": {},
   "source": [
    "Training Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class TrainState(nnx.Object):\n",
    "    params: Any\n",
    "    graph_def: Any\n",
    "    state: Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131db7c5",
   "metadata": {},
   "source": [
    "CV loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV_loss_fn(CV_batches, graph_def, params, state, dataset_parameters, alpha, gamma, lambda_):\n",
    "    Model = nnx.merge(graph_def, params, state)\n",
    "    CV_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for CV_batch in CV_batches:\n",
    "        batch_count += 1\n",
    "        graph_batch = CV_batch['graphs']\n",
    "        target_e_batch = CV_batch['target_e']\n",
    "        target_e_prime_batch = CV_batch['target_e_prime']\n",
    "\n",
    "        loss = loss_fn(\n",
    "            graph_batch,\n",
    "            target_e_batch,\n",
    "            target_e_prime_batch,\n",
    "            Model=Model,\n",
    "            Dataset_parameters=dataset_parameters,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        CV_loss += loss\n",
    "\n",
    "    CV_loss = CV_loss / batch_count\n",
    "\n",
    "    return CV_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24c632",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, graph_def, state, opt_state, GraphandTarget_batch, *, dataset_parameters, alpha, gamma, lambda_):\n",
    "\n",
    "    target_e_batch = GraphandTarget_batch['target_e']\n",
    "    target_e_prime_batch = GraphandTarget_batch['target_e_prime']\n",
    "    graph_batch = GraphandTarget_batch['graphs']\n",
    "\n",
    "    def wrapped_loss(params_, state_):\n",
    "        Model = nnx.merge(graph_def, params_, state_)\n",
    "        loss = loss_fn(\n",
    "            graph_batch,\n",
    "            target_e_batch,\n",
    "            target_e_prime_batch,\n",
    "            Model=Model,\n",
    "            Dataset_parameters=dataset_parameters,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss, argnums=0)(params, state)\n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_state = state\n",
    "    \n",
    "    return new_params, new_state, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98768043",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59280fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = GNN(\n",
    "    input_dim=node_features[0], \n",
    "    embedding_dim=128,\n",
    "    output_dim=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "input_dim: int, embedding_dim: int, output_dim: int, rngs: nnx.Rngs\n",
    "\n",
    "graph_def,params,state = nnx.split(Model,nnx.Param,nnx.State)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "train_state = TrainState(\n",
    "    graph_def=graph_def,\n",
    "    params=params,\n",
    "    state=state,\n",
    "    )\n",
    "\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for batch in tqdm(train_batches, desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "\n",
    "        new_params, new_state, new_opt_state, batch_loss = train_step(\n",
    "\n",
    "        )\n",
    "\n",
    "        CV_loss = CV_loss_fn(\n",
    "            CV_batches,\n",
    "            graph_def,\n",
    "            new_params,\n",
    "            new_state,\n",
    "            dataset_parameters,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            lambda_\n",
    "        )\n",
    "\n",
    "        opt_state = new_opt_state\n",
    "        train_state.params = new_params\n",
    "        train_state.state = new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293e5bf",
   "metadata": {},
   "source": [
    "Trained Model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1815d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.dataclass\n",
    "class Model_storage(nnx.Object):\n",
    "    params: Any\n",
    "    graph_def: Any\n",
    "    states: Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67fd43",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd42bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_WSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
