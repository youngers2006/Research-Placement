{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183dd517",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.nn as jnn\n",
    "import flax.nnx as nnx\n",
    "from flax import struct\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import Any\n",
    "import jraph\n",
    "from itertools import combinations\n",
    "import meshio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78afd1",
   "metadata": {},
   "source": [
    "Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b53ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Epochs = 500\n",
    "alpha = 1.0 ; gamma = 1.0 ; lambda_ = 1.0\n",
    "beta_1 = 0.999 ; beta_2 = 0.9\n",
    "batch_size = 20\n",
    "train_split = 0.9 ; CV_split = 0.05 ; test_split = 0.05\n",
    "Learn_Rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd846b",
   "metadata": {},
   "source": [
    "RNG key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f457fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42 # This can be changed but is here to make the results easy to reproduce\n",
    "base_key = jax.random.PRNGKey(seed)\n",
    "rngs = nnx.Rngs(base_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992108e",
   "metadata": {},
   "source": [
    "Graph gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b3203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_known(boundary_points, points):\n",
    "    is_known = jnp.zeros(points.shape[0]) \n",
    "    is_known = is_known.at[boundary_points].set(1)\n",
    "    return is_known\n",
    "\n",
    "def build_send_receive(cell):\n",
    "    sender_array = []\n",
    "    receiver_array = []\n",
    "    for edge in combinations(cell,2):\n",
    "        sender_array.append(edge[0])\n",
    "        receiver_array.append(edge[1])\n",
    "    return sender_array, receiver_array\n",
    "\n",
    "def build_graphs(senders, receivers, positions, boundary_points, U) -> jraph.GraphsTuple:\n",
    "    is_known = Get_known(boundary_points, positions)\n",
    "    U_applied = jnp.zeros_like(U).at[boundary_points].set(U[boundary_points])\n",
    "        \n",
    "    node_features = jnp.concatenate([positions, U_applied, jnp.expand_dims(is_known, axis=1)], axis=1)\n",
    "    num_nodes = positions.shape[0]\n",
    "\n",
    "    graph = jraph.GraphsTuple(\n",
    "        nodes=node_features,\n",
    "        senders=senders,\n",
    "        receivers=receivers,\n",
    "        edges=None,\n",
    "        globals=None, \n",
    "        n_node=jnp.array([num_nodes]),\n",
    "        n_edge=jnp.array([len(senders)])\n",
    "    )\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5385e",
   "metadata": {},
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab417de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete.\n",
      "\n",
      "Positions array shape: (1331, 3)\n",
      "Boundary indices array shape: (602,)\n",
      "Senders array shape: (14230,)\n",
      "Receivers array shape: (14230,)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your result file\n",
    "filepath = os.path.join('data', 'vtk', 'u_final.vtu')\n",
    "\n",
    "if not os.path.exists(filepath):\n",
    "    print(f\"Error: '{filepath}' not found. Please check the file path.\")\n",
    "else:\n",
    "    mesh = meshio.read(filepath)\n",
    "\n",
    "    positions = mesh.points\n",
    "    right_face_indices = np.where(np.isclose(positions[:, 0], 1.0))[0]\n",
    "    element_connectivity = mesh.cells[0].data\n",
    "\n",
    "    unique_edges = set()\n",
    "\n",
    "    for element in element_connectivity:\n",
    "        element_senders, element_receivers = build_send_receive(element)\n",
    "        \n",
    "        for i in range(len(element_senders)):\n",
    "            edge = tuple(sorted((element_senders[i], element_receivers[i])))\n",
    "            unique_edges.add(edge)\n",
    "\n",
    "    edge_list = jnp.array(list(unique_edges))\n",
    "    senders = edge_list[:, 0]\n",
    "    receivers = edge_list[:, 1]\n",
    "\n",
    "    on_face_x0 = np.isclose(positions[:, 0], 0.0)\n",
    "    on_face_x1 = np.isclose(positions[:, 0], 1.0)\n",
    "    on_face_y0 = np.isclose(positions[:, 1], 0.0)\n",
    "    on_face_y1 = np.isclose(positions[:, 1], 1.0)\n",
    "    on_face_z0 = np.isclose(positions[:, 2], 0.0)\n",
    "    on_face_z1 = np.isclose(positions[:, 2], 1.0)\n",
    "\n",
    "    is_on_any_face = (on_face_x0 | on_face_x1 |\n",
    "                      on_face_y0 | on_face_y1 |\n",
    "                      on_face_z0 | on_face_z1)\n",
    "\n",
    "    boundary_nodes = np.where(is_on_any_face)[0]\n",
    "\n",
    "    print(\"Data extraction complete.\\n\")\n",
    "    print(f\"Positions array shape: {positions.shape}\")\n",
    "    print(f\"Boundary indices array shape: {boundary_nodes.shape}\")\n",
    "    print(f\"Senders array shape: {senders.shape}\")\n",
    "    print(f\"Receivers array shape: {receivers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202054",
   "metadata": {},
   "source": [
    "Unpickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d8994d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully unpickled data.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import types\n",
    "import pickle\n",
    "\n",
    "fake_module = types.ModuleType(\"DataSetup\")\n",
    "\n",
    "class DataStore:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "fake_module.DataStore = DataStore\n",
    "\n",
    "sys.modules[\"DataSetup\"] = fake_module\n",
    "\n",
    "data_file = r\"/home/samuel/Github/Research-Placement/data/simulation_results.pkl\"\n",
    "\n",
    "try:\n",
    "    with open(data_file, \"rb\") as f:\n",
    "        data_unpickled_1 = pickle.load(f)\n",
    "    print(f\"Successfully unpickled data.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {data_file}\")\n",
    "    dataset_list = {}\n",
    "\n",
    "dataset_dict = data_unpickled_1\n",
    "\n",
    "# Not tunable, is known from how many sims ran\n",
    "num_sims = 20000\n",
    "# permutation list for batching\n",
    "index_list = jnp.arange(num_sims)\n",
    "permutated_index_list = jax.random.permutation(jax.random.PRNGKey(0), index_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ce7c1",
   "metadata": {},
   "source": [
    "check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d690e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1806, 3)\n",
      "(1331, 3)\n",
      "()\n",
      "(1806, 3)\n",
      "[[-2.9210228e-04 -6.3567061e-04 -1.4251155e-03]\n",
      " [-4.4415184e-04  8.9642766e-05 -6.0152575e-05]\n",
      " [-3.8378930e-04 -1.0108644e-04 -1.7809425e-05]\n",
      " ...\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00]]\n",
      "[[-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " [-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " [-3.3549329e-03 -6.5468852e-03 -1.0724306e-03]\n",
      " ...\n",
      " [ 1.5876233e-03 -1.5465567e-04 -2.3625712e-03]\n",
      " [ 1.4152135e-03 -4.2626876e-04 -3.6217444e-04]\n",
      " [ 2.9660708e-03  7.4987329e-05  4.9074343e-03]]\n",
      "0.004835612\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[0]['boundary_strain_energy_gradient'].shape)\n",
    "print(dataset_dict[0]['full_displacement_vector'].shape)\n",
    "print(dataset_dict[0]['strain_energy'].shape)\n",
    "print(dataset_dict[0]['applied_boundary_displacements'].shape)\n",
    "\n",
    "print(dataset_dict[0]['boundary_strain_energy_gradient'])\n",
    "print(dataset_dict[0]['full_displacement_vector'])\n",
    "print(dataset_dict[0]['strain_energy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183f4bc",
   "metadata": {},
   "source": [
    "Pre-processing functions - Need to be changed when preprocessing is implemented to accomodate the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c6ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std_dev(data,*, train_split, permutated_idxs):\n",
    "    split_idx = int(permutated_idxs.shape[0] * train_split)\n",
    "    train_data = data[:split_idx, :]\n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std_dev = jnp.std(train_data, axis=0)\n",
    "    return {'mean':mean, 'std_dev':std_dev}\n",
    "\n",
    "def scale_data(data,*, data_params):\n",
    "    return (data - data_params['mean']) / data_params['std_dev']\n",
    "    \n",
    "def unscale_data(data,*,data_params):\n",
    "    return (data * data_params['std_dev']) + data_params['mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a8d00",
   "metadata": {},
   "source": [
    "Data pre-processing and graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47f917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    }
   ],
   "source": [
    "processed_dataset_dict = dataset_dict\n",
    "\n",
    "graphs_list = []\n",
    "displacements_list = []\n",
    "target_e_list = []\n",
    "target_e_prime_list = []\n",
    "boundary_displacements_list = []\n",
    "\n",
    "# Graph Building\n",
    "for i in tqdm(range(num_sims), leave=False):\n",
    "    U = processed_dataset_dict[i]['full_displacement_vector']\n",
    "    U = jnp.array(U)\n",
    "    displacements_list.append(U)\n",
    "    target_e = jnp.array(processed_dataset_dict[i]['strain_energy'])\n",
    "    target_e_list.append(target_e)\n",
    "    target_e_prime = jnp.array(processed_dataset_dict[i]['boundary_strain_energy_gradient'])\n",
    "    target_e_prime_list.append(target_e_prime)\n",
    "    bdd = jnp.array(processed_dataset_dict[i]['applied_boundary_displacements'])\n",
    "    boundary_displacements_list.append(bdd)\n",
    "\n",
    "boundary_displacements_array = jnp.stack(boundary_displacements_list) # (num_sims, nodes, displacement components)\n",
    "target_e_array = jnp.stack(target_e_list) # (num_sims, value)\n",
    "target_e_prime_array = jnp.stack(target_e_prime_list)\n",
    "displacements_array = jnp.stack(displacements_list)\n",
    "\n",
    "# Pre-processing\n",
    "displacement_params = mean_and_std_dev(\n",
    "    boundary_displacements_array, \n",
    "    train_split=train_split, \n",
    "    permutated_idxs=permutated_index_list\n",
    ")\n",
    "\n",
    "target_e_params = mean_and_std_dev(\n",
    "    target_e_array, \n",
    "    train_split=train_split, \n",
    "    permutated_idxs=permutated_index_list\n",
    ")\n",
    "\n",
    "e_prime_params = mean_and_std_dev(\n",
    "    target_e_prime_array, \n",
    "    train_split=train_split, \n",
    "    permutated_idxs=permutated_index_list\n",
    ")\n",
    "\n",
    "params_dict = {\n",
    "    'displacements': displacement_params,\n",
    "    'target_e': target_e_params,\n",
    "    'target_e_prime': e_prime_params\n",
    "}\n",
    "\n",
    "graph = build_graphs(\n",
    "        senders, \n",
    "        receivers, \n",
    "        positions, \n",
    "        boundary_nodes, \n",
    "        U\n",
    "    )\n",
    "graphs_list.append(graph)\n",
    "\n",
    "dataset = {\n",
    "    'graphs_list': graphs_list,\n",
    "    'displacements': displacements_array,\n",
    "    'target_e': target_e_array,\n",
    "    'target_e_prime': target_e_prime_array,\n",
    "    'boundary_displacements': boundary_displacements_array\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511eda85",
   "metadata": {},
   "source": [
    "Batching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686d79ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_and_split_dataset(dataset_dict, batch_size, train_split, CV_split, test_split, permutated_index_list):\n",
    "    n_train_samples = int(train_split * permutated_index_list.shape[0]) \n",
    "    n_test_samples = int(test_split * permutated_index_list.shape[0]) \n",
    "    n_CV_samples = int(CV_split * permutated_index_list.shape[0]) \n",
    "\n",
    "    train_idx = list(permutated_index_list[:n_train_samples])\n",
    "    test_idx = list(permutated_index_list[n_train_samples:(n_train_samples + n_test_samples)])\n",
    "    CV_idx = list(permutated_index_list[(n_train_samples + n_test_samples):])\n",
    "\n",
    "    def batch_indices(idx):  \n",
    "        num_batches = len(idx) // batch_size\n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_idx = idx[start:end]\n",
    "            \n",
    "            graphs_in_batch = [dataset_dict['graphs_list'][i] for i in batch_idx]\n",
    "            displacements_batch = [dataset_dict['displacements'][i] for i in batch_idx]\n",
    "            e_batch = [dataset_dict['target_e'][i] for i in batch_idx]\n",
    "            e_prime_batch = [dataset_dict['target_e_prime'][i] for i in batch_idx]\n",
    "\n",
    "            batched_graphs = jraph.batch(graphs_in_batch)\n",
    "            batched_displacements = jnp.array(displacements_batch)\n",
    "            batched_e = jnp.array(e_batch)\n",
    "            batched_e_prime = jnp.array(e_prime_batch)\n",
    "\n",
    "            yield {\n",
    "                'graphs': batched_graphs, \n",
    "                'displacements': batched_displacements, \n",
    "                'target_e': batched_e, \n",
    "                'target_e_prime': batched_e_prime\n",
    "            }\n",
    "    \n",
    "    train_batches = list(batch_indices(train_idx))\n",
    "    test_batches = list(batch_indices(test_idx))\n",
    "    CV_batches = list(batch_indices(CV_idx))\n",
    "\n",
    "    return train_batches, CV_batches, test_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9a939",
   "metadata": {},
   "source": [
    "Batching graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80b07a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-28 19:34:26.227208: W external/xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 889.4KiB (rounded to 910848)requested by op \n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-08-28 19:34:26.310952: W external/xla/xla/tsl/framework/bfc_allocator.cc:512] ****************************************************************************************************\n",
      "E0828 19:34:26.311015 1840709 pjrt_stream_executor_client.cc:2939] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 910720 bytes. [tf-allocator-allocation-error='']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 910720 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_batches, CV_batches, test_batches = batch_and_split_dataset(\n\u001b[32m      2\u001b[39m     dataset, \n\u001b[32m      3\u001b[39m     batch_size, \n\u001b[32m      4\u001b[39m     train_split, \n\u001b[32m      5\u001b[39m     CV_split, \n\u001b[32m      6\u001b[39m     test_split, \n\u001b[32m      7\u001b[39m     permutated_index_list\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mbatch_and_split_dataset\u001b[39m\u001b[34m(dataset_dict, batch_size, train_split, CV_split, test_split, permutated_index_list)\u001b[39m\n\u001b[32m     25\u001b[39m         batched_e_prime = jnp.array(e_prime_batch)\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[32m     28\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgraphs\u001b[39m\u001b[33m'\u001b[39m: batched_graphs, \n\u001b[32m     29\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mdisplacements\u001b[39m\u001b[33m'\u001b[39m: batched_displacements, \n\u001b[32m     30\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtarget_e\u001b[39m\u001b[33m'\u001b[39m: batched_e, \n\u001b[32m     31\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtarget_e_prime\u001b[39m\u001b[33m'\u001b[39m: batched_e_prime\n\u001b[32m     32\u001b[39m         }\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m train_batches = \u001b[38;5;28mlist\u001b[39m(batch_indices(train_idx))\n\u001b[32m     35\u001b[39m test_batches = \u001b[38;5;28mlist\u001b[39m(batch_indices(test_idx))\n\u001b[32m     36\u001b[39m CV_batches = \u001b[38;5;28mlist\u001b[39m(batch_indices(CV_idx))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mbatch_and_split_dataset.<locals>.batch_indices\u001b[39m\u001b[34m(idx)\u001b[39m\n\u001b[32m     19\u001b[39m e_batch = [dataset_dict[\u001b[33m'\u001b[39m\u001b[33mtarget_e\u001b[39m\u001b[33m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idx]\n\u001b[32m     20\u001b[39m e_prime_batch = [dataset_dict[\u001b[33m'\u001b[39m\u001b[33mtarget_e_prime\u001b[39m\u001b[33m'\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m batch_idx]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m batched_graphs = jraph.batch(graphs_in_batch)\n\u001b[32m     23\u001b[39m batched_displacements = jnp.array(displacements_batch)\n\u001b[32m     24\u001b[39m batched_e = jnp.array(e_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jraph/_src/utils.py:477\u001b[39m, in \u001b[36mbatch\u001b[39m\u001b[34m(graphs)\u001b[39m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch\u001b[39m(graphs: Sequence[gn_graph.GraphsTuple]) -> gn_graph.GraphsTuple:\n\u001b[32m    425\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns a batched graph given a list of graphs.\u001b[39;00m\n\u001b[32m    426\u001b[39m \n\u001b[32m    427\u001b[39m \u001b[33;03m  This method will concatenate the ``nodes``, ``edges`` and ``globals``,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    475\u001b[39m \u001b[33;03m      graph.\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _batch(graphs, np_=jnp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jraph/_src/utils.py:502\u001b[39m, in \u001b[36m_batch\u001b[39m\u001b[34m(graphs, np_)\u001b[39m\n\u001b[32m    493\u001b[39m   concat = \u001b[38;5;28;01mlambda\u001b[39;00m *args: np_.concatenate(args)\n\u001b[32m    494\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m tree.tree_map(concat, *nests)\n\u001b[32m    496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m gn_graph.GraphsTuple(\n\u001b[32m    497\u001b[39m     n_node=np_.concatenate([g.n_node \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]),\n\u001b[32m    498\u001b[39m     n_edge=np_.concatenate([g.n_edge \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]),\n\u001b[32m    499\u001b[39m     nodes=_map_concat([g.nodes \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]),\n\u001b[32m    500\u001b[39m     edges=_map_concat([g.edges \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]),\n\u001b[32m    501\u001b[39m     \u001b[38;5;28mglobals\u001b[39m=_map_concat([g.globals \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m graphs]),\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     senders=np_.concatenate([g.senders + o \u001b[38;5;28;01mfor\u001b[39;00m g, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(graphs, offsets)]),\n\u001b[32m    503\u001b[39m     receivers=np_.concatenate(\n\u001b[32m    504\u001b[39m         [g.receivers + o \u001b[38;5;28;01mfor\u001b[39;00m g, o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(graphs, offsets)]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/numpy/lax_numpy.py:4626\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, dtype)\u001b[39m\n\u001b[32m   4624\u001b[39m k = \u001b[32m16\u001b[39m\n\u001b[32m   4625\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) > \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4626\u001b[39m   arrays_out = [lax.concatenate(arrays_out[i:i+k], axis)\n\u001b[32m   4627\u001b[39m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[32m   4628\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/lax/lax.py:2003\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(operands, dimension)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op\n\u001b[32m   2002\u001b[39m operands = core.standard_insert_pvary(*operands)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate_p.bind(*operands, dimension=dimension)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/core.py:576\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    575\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._true_bind(*args, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/core.py:592\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    590\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bind_with_trace(prev_trace, args, params)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    594\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/core.py:602\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    599\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m set_current_trace(trace):\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_lojax(*args, **params)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trace.process_primitive(\u001b[38;5;28mself\u001b[39m, args, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/core.py:1126\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1124\u001b[39m args = \u001b[38;5;28mmap\u001b[39m(full_lower, args)\n\u001b[32m   1125\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m primitive.impl(*args, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/JAX_ML_WSL/lib/python3.13/site-packages/jax/_src/dispatch.py:91\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     89\u001b[39m prev = lib.jax_jit.swap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m   outs = fun(*args)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m   lib.jax_jit.swap_thread_local_state_disable_jit(prev)\n",
      "\u001b[31mValueError\u001b[39m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 910720 bytes."
     ]
    }
   ],
   "source": [
    "train_batches, CV_batches, test_batches = batch_and_split_dataset(\n",
    "    dataset, \n",
    "    batch_size, \n",
    "    train_split, \n",
    "    CV_split, \n",
    "    test_split, \n",
    "    permutated_index_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c22883",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Silu(x: jax.Array) -> jax.Array:\n",
    "    return x * nnx.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615042f",
   "metadata": {},
   "source": [
    "GAT Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1030d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nnx.Module):\n",
    "    def __init__(self, in_features, out_features,*,rngs):\n",
    "        key = rngs.params()\n",
    "        initialiser = nnx.initializers.lecun_normal()\n",
    "        self.W = nnx.Param(initialiser(key=key, shape=(in_features, out_features)))\n",
    "        self.A = nnx.Param(initialiser(key=key, shape=(2 * out_features, 1)))\n",
    "\n",
    "    def __call__(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "\n",
    "        if graph.n_node is None:\n",
    "            raise ValueError(\"GAT requires nodes to have features\")\n",
    "        \n",
    "        h_sender = graph.nodes[graph.senders] @ self.W\n",
    "        h_receiver = graph.nodes[graph.receivers] @ self.W\n",
    "\n",
    "        send_receive_features = jnp.concatenate([h_sender, h_receiver], axis=-1)\n",
    "        attention_scores = nnx.leaky_relu(send_receive_features @ self.A)\n",
    "        \n",
    "        attention_coefficients = jraph.segment_softmax(\n",
    "            logits=attention_scores, \n",
    "            segments_ids=graph.receivers,\n",
    "            num_segments=graph.n_node\n",
    "        )\n",
    "\n",
    "        weighted_features = attention_coefficients * h_sender\n",
    "\n",
    "        aggregate_nodes = jraph.aggregate_edges_for_nodes(\n",
    "            graph=graph,\n",
    "            edge_features=weighted_features,\n",
    "            aggregate_fn=jnp.sum\n",
    "        )\n",
    "\n",
    "        return graph._replace(nodes=aggregate_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a045c04d",
   "metadata": {},
   "source": [
    "SAGPool WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a377c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGPool(nnx.Module): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314af953",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nnx.Module):\n",
    "    def __init__(self, node_feature_dim: int, embedding_dim: int, output_dim: int, rngs: nnx.Rngs):\n",
    "        self.embedding_layer = nnx.Linear(node_feature_dim, embedding_dim, rngs=rngs)\n",
    "        self.decoding_layer = nnx.Linear(embedding_dim, output_dim, rngs=rngs)\n",
    "        self.encoderL1 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL1 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL2 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "        self.BatchNormL2 = nnx.BatchNorm(num_features=embedding_dim, rngs=rngs)\n",
    "        self.encoderL3 = GAT(embedding_dim, embedding_dim, rngs=rngs)\n",
    "    \n",
    "    def embedder(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        embeddings = self.embedding_layer(nodes)\n",
    "        return graph._replace(nodes=embeddings)\n",
    "    \n",
    "    def apply_activation_and_res(self, graph: jraph.GraphsTuple, residual: jax.Array) -> jraph.GraphsTuple:\n",
    "        nodes = graph.nodes\n",
    "        activated_nodes = nnx.relu(nodes) + residual\n",
    "        return graph._replace(nodes=activated_nodes)\n",
    "    \n",
    "    def apply_res(self, graph: jraph.GraphsTuple, residual: jax.Array):\n",
    "        new_nodes = graph.nodes + residual\n",
    "        return graph._replace(nodes=new_nodes)\n",
    "        \n",
    "    def decoder(self, graph: jraph.GraphsTuple) -> jraph.GraphsTuple: # Switch to SAGPool when its finished\n",
    "        aggregate_nodes = jraph.aggregate_nodes(graph, jnp.sum)\n",
    "        return self.decoding_layer(aggregate_nodes)\n",
    "        \n",
    "    def forward_pass(self, G: jraph.GraphsTuple, use_running_average: bool) -> jax.Array:\n",
    "        G = self.embedder(G)\n",
    "        res1 = G.nodes\n",
    "\n",
    "        G = self.encoderL1(G)\n",
    "        self.BatchNormL1.use_running_average = use_running_average\n",
    "        nodes_norm = self.BatchNormL1(G.nodes)\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res1)\n",
    "        res2 = G.nodes\n",
    "\n",
    "        G = self.encoderL2(G)\n",
    "        self.BatchNormL2.use_running_average = use_running_average\n",
    "        nodes_norm = self.BatchNormL2(G.nodes)\n",
    "        G = G._replace(nodes=nodes_norm)\n",
    "        G = self.apply_activation_and_res(G, res2)\n",
    "        res3 = G.nodes\n",
    "\n",
    "        G = self.encoderL3(G)\n",
    "        G = self.apply_res(G, res3)\n",
    "\n",
    "        e = self.decoder(G)\n",
    "        return e\n",
    "    \n",
    "    def __call__(self, G: jraph.GraphsTuple, use_running_average, boundary_nodes_idx):\n",
    "\n",
    "        e = self.forward_pass(G, use_running_average)\n",
    "        grad_graph = jax.grad(self.forward_pass, argnums=0)(G, use_running_average)\n",
    "        e_prime = grad_graph.nodes[:,4:7]\n",
    "\n",
    "        return e, e_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9615288",
   "metadata": {},
   "source": [
    "Loss function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optimiser Instance\n",
    "optimiser = optax.chain(\n",
    "    optax.add_decayed_weights(weight_decay=1e-5),\n",
    "    optax.adam(\n",
    "        learning_rate=Learn_Rate, \n",
    "        b1=beta_1, \n",
    "        b2=beta_2\n",
    "    )\n",
    ")\n",
    "\n",
    "def loss_fn(graph_batch, target_e, target_e_prime,*, Model, alpha, gamma, lam): \n",
    "    \"\"\"\n",
    "    Calculates the loss of a model, works to minimise the mean square error of both \n",
    "    the strain energy prediction and the strain energy derivative prediction,\n",
    "    whilst forcing the function through zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_e, prediction_e_prime = Model(graph_batch)\n",
    "    loss_e = jnp.mean((prediction_e - target_e)**2)\n",
    "    loss_e_prime = jnp.mean((prediction_e_prime - target_e_prime)**2)\n",
    "    \n",
    "    is_known = Get_known(boundary_nodes, positions)\n",
    "    U_zero = jnp.zeros_like(positions)\n",
    "    node_features = jnp.concatenate([positions, U_zero, is_known], axis=1)\n",
    "    zero_graph = jraph.GraphsTuple(\n",
    "        nodes=node_features,\n",
    "        senders=senders,\n",
    "        receivers=receivers,\n",
    "        edges=None,\n",
    "        globals=None, \n",
    "        n_node=jnp.array([positions.shape[0]]),\n",
    "        n_edge=jnp.array([len(senders)])\n",
    "    )\n",
    "    \n",
    "    prediction_zero, prediction_zero_prime = Model(zero_graph)\n",
    "    target_e_prime_zero = jnp.zeros_like(target_e_prime[0])\n",
    "    loss_zero = jnp.mean((prediction_zero - 0)**2) + jnp.mean(optax.huber_loss(prediction_zero_prime, target_e_prime_zero))\n",
    "\n",
    "\n",
    "    return (alpha * loss_e + gamma * loss_e_prime + lam * loss_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd37a6b",
   "metadata": {},
   "source": [
    "Training Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(nnx.Object):\n",
    "    def __init__(self,params,graph_def,state):\n",
    "        self.params = params\n",
    "        self.graph_def = graph_def\n",
    "        self.state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131db7c5",
   "metadata": {},
   "source": [
    "CV loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def CV_loss_fn(CV_batches, graph_def, params, state, alpha, gamma, lambda_):\n",
    "    Model = nnx.merge(graph_def, params, state)\n",
    "    CV_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for CV_batch in CV_batches:\n",
    "        batch_count += 1\n",
    "        graph_batch = CV_batch['graphs']\n",
    "        target_e_batch = CV_batch['target_e']\n",
    "        target_e_prime_batch = CV_batch['target_e_prime']\n",
    "\n",
    "        loss = loss_fn(\n",
    "            graph_batch,\n",
    "            target_e_batch,\n",
    "            target_e_prime_batch,\n",
    "            Model=Model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_\n",
    "        )\n",
    "\n",
    "        CV_loss += loss\n",
    "\n",
    "    if batch_count > 0:\n",
    "        CV_loss = CV_loss / batch_count\n",
    "        return CV_loss\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24c632",
   "metadata": {},
   "source": [
    "Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, graph_def, state, opt_state, GraphandTarget_batch, *, alpha, gamma, lambda_):\n",
    "\n",
    "    target_e_batch = GraphandTarget_batch['target_e']\n",
    "    target_e_prime_batch = GraphandTarget_batch['target_e_prime']\n",
    "    graph_batch = GraphandTarget_batch['graphs']\n",
    "\n",
    "    def wrapped_loss(params_, state_):\n",
    "        Model = nnx.merge(graph_def, params_, state_)\n",
    "        loss = loss_fn(\n",
    "            graph_batch,\n",
    "            target_e_batch,\n",
    "            target_e_prime_batch,\n",
    "            Model=Model,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lam=lambda_ \n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(wrapped_loss, argnums=0)(params, state)\n",
    "    updates, new_opt_state = optimiser.update(grads, opt_state)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    new_state = state\n",
    "    \n",
    "    return new_params, new_state, new_opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98768043",
   "metadata": {},
   "source": [
    "Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59280fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                               \r"
     ]
    }
   ],
   "source": [
    "# Instantiate energy prediction NN\n",
    "Model = GNN(\n",
    "    node_feature_dim=7, \n",
    "    embedding_dim=128,\n",
    "    output_dim=1,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "graph_def,params,state = nnx.split(Model,nnx.Param,nnx.BatchStat)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "train_state = TrainState(\n",
    "    graph_def=graph_def,\n",
    "    params=params,\n",
    "    state=state\n",
    ")\n",
    "\n",
    "loss_record = []\n",
    "CV_loss_record = []\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    running_loss = 0.0\n",
    "    running_CV_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for batch in tqdm(train_batches, desc=f\"Epoch {epoch}/{Epochs}\", leave=False):\n",
    "\n",
    "        new_params, new_state, new_opt_state, batch_loss = train_step(\n",
    "            train_state.params,\n",
    "            train_state.graph_def,\n",
    "            train_state.state,\n",
    "            opt_state,\n",
    "            batch,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,\n",
    "            lambda_=lambda_\n",
    "        )\n",
    "\n",
    "        opt_state = new_opt_state\n",
    "        train_state.params = new_params\n",
    "        train_state.state = new_state\n",
    "\n",
    "        batch_count += 1\n",
    "        running_loss += batch_loss\n",
    "\n",
    "    CV_loss = CV_loss_fn(\n",
    "        CV_batches,\n",
    "        train_state.graph_def,\n",
    "        train_state.params,\n",
    "        train_state.state,\n",
    "        alpha,\n",
    "        gamma,\n",
    "        lambda_\n",
    "    )\n",
    "    \n",
    "    loss_record.append(running_loss)\n",
    "    CV_loss_record.append(CV_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6402f7aa",
   "metadata": {},
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65db5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ceb0dbc5a90>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293e5bf",
   "metadata": {},
   "source": [
    "Trained Model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1815d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_trained(nnx.Object):\n",
    "    def __init__(self, params, graph_def, states):\n",
    "        self.params = params\n",
    "        self.graph_def = graph_def\n",
    "        self.states = states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JAX_ML_WSL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
